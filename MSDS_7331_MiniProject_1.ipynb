{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 Mini - Project 1  \n",
    "\n",
    "## SVM and Logistic Modeling\n",
    "\n",
    "Professor: Dr. Jake Drew  \n",
    "Team: Steven Hayden, Josephine MacDaniel, Korey MacVittie, Afreen Siddiqui, Eduardo Cantu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp1\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "Accident_df_2016 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2016.csv',low_memory=False)\n",
    "Accident_df_2015 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2015.csv',low_memory=False)\n",
    "Accident_df = pd.concat([Accident_df_2015,Accident_df_2016])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66978 entries, 0 to 34438\n",
      "Data columns (total 71 columns):\n",
      "Unnamed: 0                                                     66978 non-null int64\n",
      "state_number                                                   66978 non-null int64\n",
      "state_name                                                     66978 non-null object\n",
      "consecutive_number                                             66978 non-null int64\n",
      "number_of_vehicle_forms_submitted_all                          66978 non-null int64\n",
      "number_of_motor_vehicles_in_transport_mvit                     66978 non-null int64\n",
      "number_of_parked_working_vehicles                              66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_not_in_motor_vehicles    66978 non-null int64\n",
      "number_of_persons_not_in_motor_vehicles_in_transport_mvit      66978 non-null int64\n",
      "number_of_persons_in_motor_vehicles_in_transport_mvit          66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_in_motor_vehicles        66978 non-null int64\n",
      "county                                                         66978 non-null int64\n",
      "city                                                           66978 non-null int64\n",
      "day_of_crash                                                   66978 non-null int64\n",
      "month_of_crash                                                 66978 non-null int64\n",
      "year_of_crash                                                  66978 non-null int64\n",
      "day_of_week                                                    66978 non-null int64\n",
      "hour_of_crash                                                  66978 non-null int64\n",
      "minute_of_crash                                                66978 non-null int64\n",
      "national_highway_system                                        66978 non-null int64\n",
      "land_use                                                       66978 non-null int64\n",
      "land_use_name                                                  66978 non-null object\n",
      "functional_system                                              66978 non-null int64\n",
      "functional_system_name                                         66978 non-null object\n",
      "ownership                                                      66978 non-null int64\n",
      "ownership_name                                                 66978 non-null object\n",
      "route_signing                                                  66978 non-null int64\n",
      "route_signing_name                                             66978 non-null object\n",
      "trafficway_identifier                                          66978 non-null object\n",
      "trafficway_identifier_2                                        17686 non-null object\n",
      "milepoint                                                      66978 non-null int64\n",
      "latitude                                                       66978 non-null float64\n",
      "longitude                                                      66978 non-null float64\n",
      "special_jurisdiction                                           66978 non-null int64\n",
      "special_jurisdiction_name                                      66978 non-null object\n",
      "first_harmful_event                                            66978 non-null int64\n",
      "first_harmful_event_name                                       66978 non-null object\n",
      "manner_of_collision                                            66978 non-null int64\n",
      "manner_of_collision_name                                       66978 non-null object\n",
      "relation_to_junction_within_interchange_area                   66978 non-null object\n",
      "relation_to_junction_specific_location                         66978 non-null int64\n",
      "relation_to_junction_specific_location_name                    66978 non-null object\n",
      "type_of_intersection                                           66978 non-null object\n",
      "work_zone                                                      66978 non-null object\n",
      "relation_to_trafficway                                         66978 non-null int64\n",
      "relation_to_trafficway_name                                    66978 non-null object\n",
      "light_condition                                                66978 non-null int64\n",
      "light_condition_name                                           66978 non-null object\n",
      "atmospheric_conditions_1                                       66978 non-null int64\n",
      "atmospheric_conditions_1_name                                  66978 non-null object\n",
      "atmospheric_conditions_2                                       66978 non-null int64\n",
      "atmospheric_conditions_2_name                                  66978 non-null object\n",
      "atmospheric_conditions                                         66978 non-null int64\n",
      "atmospheric_conditions_name                                    66978 non-null object\n",
      "school_bus_related                                             66978 non-null object\n",
      "rail_grade_crossing_identifier                                 66978 non-null object\n",
      "hour_of_notification                                           66978 non-null int64\n",
      "minute_of_notification                                         66978 non-null int64\n",
      "hour_of_arrival_at_scene                                       66978 non-null int64\n",
      "minute_of_arrival_at_scene                                     66978 non-null int64\n",
      "hour_of_ems_arrival_at_hospital                                66978 non-null int64\n",
      "minute_of_ems_arrival_at_hospital                              66978 non-null int64\n",
      "related_factors_crash_level_1                                  66978 non-null int64\n",
      "related_factors_crash_level_1_name                             66978 non-null object\n",
      "related_factors_crash_level_2                                  66978 non-null int64\n",
      "related_factors_crash_level_2_name                             66978 non-null object\n",
      "related_factors_crash_level_3                                  66978 non-null int64\n",
      "related_factors_crash_level_3_name                             66978 non-null object\n",
      "number_of_fatalities                                           66978 non-null int64\n",
      "number_of_drunk_drivers                                        66978 non-null int64\n",
      "timestamp_of_crash                                             66978 non-null object\n",
      "dtypes: float64(2), int64(44), object(25)\n",
      "memory usage: 36.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Displays the record count of non-null Values per attribute and their data type. \n",
    "Accident_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record. \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicate records and keeps the most recent record. We assume the most resent is the most accurate \n",
    "Accident_df = Accident_df.drop_duplicates(['consecutive_number'],keep = 'last')\n",
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record . \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many accidents with missing data regarding the crash time and the arrival time of responders. This information is need for the dependent variable and in turn is crucial for our analysis. That is why it was decided to drop these records with missing data instead of filling the gaps with the mean.  The amount records dropped is about a third of the original data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19149 records were removed due to missing time data.\n"
     ]
    }
   ],
   "source": [
    "#Remove any recoreds without  time informaiton for arival and crash time\n",
    "count_no_rec= Accident_df[(Accident_df['hour_of_crash']>24) | (Accident_df['hour_of_arrival_at_scene']>24)]\n",
    "Accident_df = Accident_df[(Accident_df['hour_of_crash']<=24) & (Accident_df['hour_of_arrival_at_scene']<=24)]\n",
    "print(count_no_rec.consecutive_number.count(), 'records were removed due to missing time data.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_of_crash</th>\n",
       "      <th>minute_of_crash</th>\n",
       "      <th>Crash_Time</th>\n",
       "      <th>hour_of_arrival_at_scene</th>\n",
       "      <th>minute_of_arrival_at_scene</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Response_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>20:00:00</td>\n",
       "      <td>00:47:00</td>\n",
       "      <td>20:47:00</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>21:01:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>19:10:00</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:24:00</td>\n",
       "      <td>19:24:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:18:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:25:00</td>\n",
       "      <td>14:25:00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:23:00</td>\n",
       "      <td>23:23:00</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:29:00</td>\n",
       "      <td>23:29:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>16:01:00</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:12:00</td>\n",
       "      <td>16:12:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour_of_crash minute_of_crash Crash_Time hour_of_arrival_at_scene  \\\n",
       "196      20:00:00        00:47:00   20:47:00                 21:00:00   \n",
       "240      19:00:00        00:10:00   19:10:00                 19:00:00   \n",
       "274      14:00:00        00:18:00   14:18:00                 14:00:00   \n",
       "312      23:00:00        00:23:00   23:23:00                 23:00:00   \n",
       "959      16:00:00        00:01:00   16:01:00                 16:00:00   \n",
       "\n",
       "    minute_of_arrival_at_scene Arrival_Time  Response_Time  \n",
       "196                   00:01:00     21:01:00             14  \n",
       "240                   00:24:00     19:24:00             14  \n",
       "274                   00:25:00     14:25:00              7  \n",
       "312                   00:29:00     23:29:00              6  \n",
       "959                   00:12:00     16:12:00             11  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new Feature. This is the ratio between the number of fatalities and the people involved in the accident.\n",
    "Accident_df['Fatalities_ratio'] = Accident_df.number_of_fatalities/(Accident_df.number_of_persons_not_in_motor_vehicles_in_transport_mvit + Accident_df.number_of_persons_in_motor_vehicles_in_transport_mvit)\n",
    "\n",
    "#Converts hour and min to datetime type\n",
    "#crash\n",
    "Accident_df.hour_of_crash = pd.to_timedelta(Accident_df.hour_of_crash,unit ='h')\n",
    "Accident_df.minute_of_crash= pd.to_timedelta(Accident_df.minute_of_crash,unit ='m')\n",
    "#arrival\n",
    "Accident_df.hour_of_arrival_at_scene = pd.to_timedelta(Accident_df.hour_of_arrival_at_scene,unit ='h')\n",
    "Accident_df.minute_of_arrival_at_scene = pd.to_timedelta(Accident_df.minute_of_arrival_at_scene,unit ='m')\n",
    "\n",
    "#concatenates Hour and Minutes together \n",
    "Accident_df['Crash_Time'] = Accident_df['hour_of_crash'] + Accident_df['minute_of_crash'] \n",
    "Accident_df['Arrival_Time'] = Accident_df['hour_of_arrival_at_scene'] + Accident_df['minute_of_arrival_at_scene']\n",
    "#creates a response_time variable from the two fields above and converts to min\n",
    "Accident_df['Response_Time'] = Accident_df['Arrival_Time'] - Accident_df['Crash_Time']\n",
    "total_response_time_in_min = pd.DatetimeIndex(Accident_df['Response_Time'])\n",
    "Accident_df['Response_Time']= total_response_time_in_min.hour * 60 + total_response_time_in_min.minute\n",
    "\n",
    "\n",
    "#gut check of calculation \n",
    "Accident_df[['hour_of_crash','minute_of_crash','Crash_Time','hour_of_arrival_at_scene','minute_of_arrival_at_scene','Arrival_Time','Response_Time']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The National Fire Protection Association's (NFPA) has established standard for response time and procedures for Emergency Medical Service (EMS) to adhere to. \n",
    "\n",
    "From EMSword.com \"The NFPA 1710 standard is based upon a combination of accepted practices and more than 30 years of study, research, testing and validation. Members of the 1710 committee that developed the standard include representatives from various fire agencies and the International Association of City/County Managers (ICMA).\"\n",
    "\n",
    "The NFPA 1710 standard allows for a one-minute call evaluation and preparation, four minutes for the arrival of a unit with first responder. For a situation that requires an advanced life support equipment like an ambulance, their standard is 8 minutes after the call preparation.  \n",
    "\n",
    "We chose to use their standards as threshold to determine if the paramedics got to the scene of the accident in time. This would be a binary response 0, for not arriving within 9 minutes of the accident and 1 for being within the 9 minutes. \n",
    "Source: https://www.emsworld.com/article/10324786/ems-response-time-standards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the binary variable 'within 9 minutes NFPA standard'\n",
    "Accident_df['within 9 minutes NFPA standard'] = np.where(Accident_df['Response_Time']<=9,1,0)\n",
    "#Accident_df[['Response_Time','within 9 minutes NFPA standard']].head(10) # Verify the binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the logistic regresion will predict if the paramedic got on time to the scene of the accident we would select the attributes that we think affect this variable.\n",
    "\n",
    "First we would check what columns are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16230 entries, 196 to 34438\n",
      "Data columns (total 76 columns):\n",
      "Unnamed: 0                                                     16230 non-null int64\n",
      "state_number                                                   16230 non-null int64\n",
      "state_name                                                     16230 non-null object\n",
      "consecutive_number                                             16230 non-null int64\n",
      "number_of_vehicle_forms_submitted_all                          16230 non-null int64\n",
      "number_of_motor_vehicles_in_transport_mvit                     16230 non-null int64\n",
      "number_of_parked_working_vehicles                              16230 non-null int64\n",
      "number_of_forms_submitted_for_persons_not_in_motor_vehicles    16230 non-null int64\n",
      "number_of_persons_not_in_motor_vehicles_in_transport_mvit      16230 non-null int64\n",
      "number_of_persons_in_motor_vehicles_in_transport_mvit          16230 non-null int64\n",
      "number_of_forms_submitted_for_persons_in_motor_vehicles        16230 non-null int64\n",
      "county                                                         16230 non-null int64\n",
      "city                                                           16230 non-null int64\n",
      "day_of_crash                                                   16230 non-null int64\n",
      "month_of_crash                                                 16230 non-null int64\n",
      "year_of_crash                                                  16230 non-null int64\n",
      "day_of_week                                                    16230 non-null int64\n",
      "hour_of_crash                                                  16230 non-null timedelta64[ns]\n",
      "minute_of_crash                                                16230 non-null timedelta64[ns]\n",
      "national_highway_system                                        16230 non-null int64\n",
      "land_use                                                       16230 non-null int64\n",
      "land_use_name                                                  16230 non-null object\n",
      "functional_system                                              16230 non-null int64\n",
      "functional_system_name                                         16230 non-null object\n",
      "ownership                                                      16230 non-null int64\n",
      "ownership_name                                                 16230 non-null object\n",
      "route_signing                                                  16230 non-null int64\n",
      "route_signing_name                                             16230 non-null object\n",
      "trafficway_identifier                                          16230 non-null object\n",
      "trafficway_identifier_2                                        4119 non-null object\n",
      "milepoint                                                      16230 non-null int64\n",
      "latitude                                                       16230 non-null float64\n",
      "longitude                                                      16230 non-null float64\n",
      "special_jurisdiction                                           16230 non-null int64\n",
      "special_jurisdiction_name                                      16230 non-null object\n",
      "first_harmful_event                                            16230 non-null int64\n",
      "first_harmful_event_name                                       16230 non-null object\n",
      "manner_of_collision                                            16230 non-null int64\n",
      "manner_of_collision_name                                       16230 non-null object\n",
      "relation_to_junction_within_interchange_area                   16230 non-null object\n",
      "relation_to_junction_specific_location                         16230 non-null int64\n",
      "relation_to_junction_specific_location_name                    16230 non-null object\n",
      "type_of_intersection                                           16230 non-null object\n",
      "work_zone                                                      16230 non-null object\n",
      "relation_to_trafficway                                         16230 non-null int64\n",
      "relation_to_trafficway_name                                    16230 non-null object\n",
      "light_condition                                                16230 non-null int64\n",
      "light_condition_name                                           16230 non-null object\n",
      "atmospheric_conditions_1                                       16230 non-null int64\n",
      "atmospheric_conditions_1_name                                  16230 non-null object\n",
      "atmospheric_conditions_2                                       16230 non-null int64\n",
      "atmospheric_conditions_2_name                                  16230 non-null object\n",
      "atmospheric_conditions                                         16230 non-null int64\n",
      "atmospheric_conditions_name                                    16230 non-null object\n",
      "school_bus_related                                             16230 non-null object\n",
      "rail_grade_crossing_identifier                                 16230 non-null object\n",
      "hour_of_notification                                           16230 non-null int64\n",
      "minute_of_notification                                         16230 non-null int64\n",
      "hour_of_arrival_at_scene                                       16230 non-null timedelta64[ns]\n",
      "minute_of_arrival_at_scene                                     16230 non-null timedelta64[ns]\n",
      "hour_of_ems_arrival_at_hospital                                16230 non-null int64\n",
      "minute_of_ems_arrival_at_hospital                              16230 non-null int64\n",
      "related_factors_crash_level_1                                  16230 non-null int64\n",
      "related_factors_crash_level_1_name                             16230 non-null object\n",
      "related_factors_crash_level_2                                  16230 non-null int64\n",
      "related_factors_crash_level_2_name                             16230 non-null object\n",
      "related_factors_crash_level_3                                  16230 non-null int64\n",
      "related_factors_crash_level_3_name                             16230 non-null object\n",
      "number_of_fatalities                                           16230 non-null int64\n",
      "number_of_drunk_drivers                                        16230 non-null int64\n",
      "timestamp_of_crash                                             16230 non-null object\n",
      "Fatalities_ratio                                               16230 non-null float64\n",
      "Crash_Time                                                     16230 non-null timedelta64[ns]\n",
      "Arrival_Time                                                   16230 non-null timedelta64[ns]\n",
      "Response_Time                                                  16230 non-null int64\n",
      "within 9 minutes NFPA standard                                 16230 non-null int32\n",
      "dtypes: float64(3), int32(1), int64(41), object(25), timedelta64[ns](6)\n",
      "memory usage: 9.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#Displays all the available attributes on the dataset\n",
    "Accident_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the attributes in the dataset we would only select a subset to predict the if the paramedics will arrive on time or not.  \n",
    "* **state_name:** This attribute can give us an indication if there are states that would have a better response time in an accident than others.\n",
    "* **route_signing_name:** This can give us an insigth on the type of roads where the paramedics find more challenging to get.  \n",
    "* **light_condition_name:** We want to underdstand how much the lightning coditions affect the ability of the paramedics to get on time to the scene of the accident.  \n",
    "* **atmospheric_conditions_name:** Adding this attribute, we want to see how the weather conditions affect the reaction time of the paramedics.  \n",
    "* **within 9 minutes NFPA standard:**  This is our response variable, where 1 means that the paramedics get on time to the scene of the accident. As mentioned before, this threshold is set by the NFPA and is 9 minutes. This field will be 0 if the paramedics take more than 9 minutes to get the the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_name',\n",
       " 'route_signing_name',\n",
       " 'light_condition_name',\n",
       " 'atmospheric_conditions_name',\n",
       " 'within 9 minutes NFPA standard']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a copy of the original Data\n",
    "Accident_forLr=Accident_df.copy()\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "for col in ['Unnamed: 0',\n",
    "             'state_number',\n",
    "             'consecutive_number',\n",
    "             'county',\n",
    "             'city',\n",
    "             'day_of_crash',\n",
    "             'month_of_crash',\n",
    "             'year_of_crash',\n",
    "             'day_of_week',\n",
    "             'hour_of_crash',\n",
    "             'minute_of_crash',\n",
    "             'national_highway_system',\n",
    "             'land_use',\n",
    "             'land_use_name',\n",
    "             'functional_system',\n",
    "             'functional_system_name',\n",
    "             'ownership',\n",
    "             'ownership_name',\n",
    "             'route_signing',\n",
    "             'trafficway_identifier',\n",
    "             'trafficway_identifier_2',\n",
    "             'latitude',\n",
    "             'longitude',\n",
    "             'special_jurisdiction',\n",
    "             'special_jurisdiction_name',\n",
    "             'first_harmful_event',\n",
    "             'first_harmful_event_name',\n",
    "             'manner_of_collision',\n",
    "             'manner_of_collision_name',\n",
    "             'relation_to_junction_within_interchange_area',\n",
    "             'relation_to_junction_specific_location',\n",
    "             'relation_to_junction_specific_location_name',\n",
    "             'type_of_intersection',\n",
    "             'work_zone',\n",
    "             'relation_to_trafficway',\n",
    "             'relation_to_trafficway_name',\n",
    "             'light_condition',\n",
    "             'atmospheric_conditions_1',\n",
    "             'atmospheric_conditions_1_name',\n",
    "             'atmospheric_conditions_2',\n",
    "             'atmospheric_conditions_2_name',\n",
    "             'atmospheric_conditions',\n",
    "             'school_bus_related',\n",
    "             'rail_grade_crossing_identifier',\n",
    "             'hour_of_notification',\n",
    "             'minute_of_notification',\n",
    "             'hour_of_arrival_at_scene',\n",
    "             'minute_of_arrival_at_scene',\n",
    "             'hour_of_ems_arrival_at_hospital',\n",
    "             'minute_of_ems_arrival_at_hospital',\n",
    "             'related_factors_crash_level_1',\n",
    "             'related_factors_crash_level_1_name',\n",
    "             'related_factors_crash_level_2',\n",
    "             'related_factors_crash_level_2_name',\n",
    "             'related_factors_crash_level_3',\n",
    "             'related_factors_crash_level_3_name',\n",
    "             'milepoint',\n",
    "             'number_of_parked_working_vehicles',\n",
    "             'number_of_forms_submitted_for_persons_not_in_motor_vehicles',\n",
    "             'number_of_persons_not_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_persons_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_forms_submitted_for_persons_in_motor_vehicles',\n",
    "             'timestamp_of_crash',\n",
    "             'number_of_fatalities',\n",
    "             'number_of_drunk_drivers',\n",
    "             'Fatalities_ratio',\n",
    "             'number_of_vehicle_forms_submitted_all',\n",
    "             'Crash_Time',\n",
    "             'Arrival_Time',\n",
    "             'Response_Time',\n",
    "             'number_of_motor_vehicles_in_transport_mvit']:\n",
    "                    if col in Accident_forLr:\n",
    "                        del Accident_forLr[col]\n",
    "# List the columns left in the df\n",
    "list(Accident_forLr)\n",
    "\n",
    "#Accident_forLr.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any observation value on the selected attributes that is reported as *Unknown, Other, or Not Reported* would be removed from the dataset. This values do not bring any value when it comes to predict the response time of the paramedics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for 'route_signing_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Unknown' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Not Reported' 'Fog, Smog, Smoke' 'Severe Crosswinds'\n",
      " 'Freezing Rain or Drizzle' 'Blowing Sand, Soil, Dirt' 'Other'] \n",
      "\n",
      "Unique values for 'atmospheric_conditions_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Unknown' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Not Reported' 'Fog, Smog, Smoke' 'Severe Crosswinds'\n",
      " 'Freezing Rain or Drizzle' 'Blowing Sand, Soil, Dirt' 'Other'] \n",
      "\n",
      "Unique values for 'light_condition_name': ['Dark – Not Lighted' 'Daylight' 'Dark – Lighted' 'Dusk' 'Dawn'\n",
      " 'Dark – Unknown Lighting' 'Not Reported' 'Unknown' 'Other']\n"
     ]
    }
   ],
   "source": [
    "# Check for columns that might not be needed\n",
    "print(\"Unique values for 'route_signing_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'atmospheric_conditions_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'light_condition_name':\" , Accident_forLr.light_condition_name.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_name                        14706\n",
       "route_signing_name                14706\n",
       "light_condition_name              14706\n",
       "atmospheric_conditions_name       14706\n",
       "within 9 minutes NFPA standard    14706\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes observations with an unknown informaiton in the route, atmospheric conditions and state name\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Not Reported')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Not Reported')]\n",
    "Accident_forLr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review that the observations have been removed from the dataset. The undesired obsrvations are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for 'route_signing_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Fog, Smog, Smoke' 'Severe Crosswinds' 'Freezing Rain or Drizzle'\n",
      " 'Blowing Sand, Soil, Dirt'] \n",
      "\n",
      "Unique values for 'atmospheric_conditions_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Fog, Smog, Smoke' 'Severe Crosswinds' 'Freezing Rain or Drizzle'\n",
      " 'Blowing Sand, Soil, Dirt'] \n",
      "\n",
      "Unique values for 'light_condition_name': ['Dark – Not Lighted' 'Daylight' 'Dark – Lighted' 'Dusk' 'Dawn'\n",
      " 'Dark – Unknown Lighting']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values for 'route_signing_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'atmospheric_conditions_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'light_condition_name':\" , Accident_forLr.light_condition_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange colums\n",
    "Accident_forLr=Accident_forLr[['within 9 minutes NFPA standard','state_name','route_signing_name','atmospheric_conditions_name','light_condition_name']]\n",
    "#list(Accident_forLr)  # Check for the correct column sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below will perform the one-hot encoding of the variables on the dataset. This is to prepare the data in such a way that can be use for our logistic regresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding of the categorical data \"state_name\"\n",
    "tmp_state_df = pd.get_dummies(Accident_forLr.state_name,prefix='state')\n",
    "#tmp_state_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"route_signing_name\"\n",
    "tmp_route_df = pd.get_dummies(Accident_forLr.route_signing_name,prefix='route')\n",
    "#tmp_route_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"atmospheric_conditions_name\"\n",
    "tmp_atmos_df = pd.get_dummies(Accident_forLr.atmospheric_conditions_name,prefix='atmos')\n",
    "#tmp_atmos_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"atmospheric_conditions_name\"\n",
    "tmp_light_df = pd.get_dummies(Accident_forLr.light_condition_name, prefix='light')\n",
    "#tmp_atmos_df.head(20)\n",
    "\n",
    "Accident_forLr = pd.concat((Accident_forLr,tmp_state_df,tmp_route_df,tmp_atmos_df,tmp_light_df), axis=1) # add back into the dataframe\n",
    "list(Accident_forLr)\n",
    "#delete the categorical variable columns\n",
    "del Accident_forLr['state_name']\n",
    "del Accident_forLr['route_signing_name']\n",
    "del Accident_forLr['atmospheric_conditions_name']\n",
    "del Accident_forLr['light_condition_name']\n",
    "\n",
    "#list(Accident_forLr) # Check for the last colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accident_forLr.count() # Count Records and show columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split and Cross Validation Setup\n",
    "In this logistic regresion we would be splitting our dataset into a Training and Test set. The split is going to be 80 % for training and the other 20 % for testing. The ratio used is accepted split in the research community, and we did not see any reason to deviate from this ratio. A Cross Validation of ten fold would be performed to validate the model. \n",
    "\n",
    "* Binary Response: *'within 9 minutes NFPA standard'*\n",
    "\n",
    "|Value|Description|\n",
    "|-----|:-----------|\n",
    "|0| Time of arrival > 9 minutes|\n",
    "|1| Time of arrival <=9 minutes|\n",
    "\n",
    "* To have the same random test and traning set each time the cross validation is perform we would use a random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'within 9 minutes NFPA standard' in Accident_forLr:\n",
    "    y = Accident_forLr['within 9 minutes NFPA standard'].values # get the labels we want\n",
    "    del Accident_forLr['within 9 minutes NFPA standard'] # get rid of the class label\n",
    "    X = Accident_forLr.values # use everything else to predict!\n",
    "    \n",
    "        ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "        #    have converted them into simple matrices to use with scikit learn\n",
    "\n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 10  #number of Cross Validation folds\n",
    "num_instances = len(y)\n",
    "\n",
    "# Cross Validation Object\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(X) # This prints all the dependant variables\n",
    "print(cv_object) # This print the Data split object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model Setup  \n",
    "This will use the cv_object to run the cross validation for the logistic regression on the *within 9 minutes NFPA standard* binary varable. At this point the data has been split into training and test. \n",
    "\n",
    "For this dataset there is an unbalance in the binary response variable. There are 1615 positive values and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.6390210740992522\n",
      "confusion matrix\n",
      " [[1233  346]\n",
      " [ 716  647]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6461590754588715\n",
      "confusion matrix\n",
      " [[1307  327]\n",
      " [ 714  594]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6573759347382733\n",
      "confusion matrix\n",
      " [[1298  331]\n",
      " [ 677  636]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.6546566961250849\n",
      "confusion matrix\n",
      " [[1313  361]\n",
      " [ 655  613]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.6515975526852481\n",
      "confusion matrix\n",
      " [[1264  376]\n",
      " [ 649  653]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.6380013596193066\n",
      "confusion matrix\n",
      " [[1242  346]\n",
      " [ 719  635]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.6437797416723318\n",
      "confusion matrix\n",
      " [[1267  376]\n",
      " [ 672  627]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.6397008837525493\n",
      "confusion matrix\n",
      " [[1281  355]\n",
      " [ 705  601]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.6363018354860639\n",
      "confusion matrix\n",
      " [[1250  398]\n",
      " [ 672  622]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.6420802175390891\n",
      "confusion matrix\n",
      " [[1261  354]\n",
      " [ 699  628]]\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " ==========================\n",
      "average accuracy 0.6448674371176071\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=0.25, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "acc_List = []\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    OnTime_lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = OnTime_lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    acc_List.append(acc)\n",
    "    iter_num+=1\n",
    "\n",
    "    \n",
    "# The Code above randomly creates a new training and testing set\n",
    "# so there will multiple accuracy measurements\n",
    "# We have taken the of all these measurements below\n",
    "\n",
    "print('\\n','\\n','\\n','\\n',\"==========================\") \n",
    "print(\"average accuracy\",np.mean(acc_List))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The mean of the 10-fold cross validation is indicated above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this section we will perform a serch for the best model using *Grid Search*. A range of parameters is given and search algorithm will try all parameter conbinations.\n",
    "For this large dataset, the solver option in the model would use *sag* to have it perform faster than the *liblinear*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 56 candidates, totalling 560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=8)]: Done 434 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=8)]: Done 560 out of 560 | elapsed:   38.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=0.25, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'penalty': ['l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'class_weight': ['balanced', None], 'random_state': [0], 'solver': ['sag', 'liblinear'], 'max_iter': [100, 500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code addapted from the notebook 'ComparingSegregatedHighSchoolCampuses.ipynb' to make a grid search for the best model\n",
    "parameters = { 'penalty':['l2']\n",
    "              ,'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "              ,'class_weight': ['balanced', None]\n",
    "              ,'random_state': [0]\n",
    "              ,'solver': ['sag','liblinear']\n",
    "              ,'max_iter':[100,500]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=OnTime_lr_clf\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv_object # KFolds = 10\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "# Perform the seaarch throught all the parameters in the parameters dictionary\n",
    "regGridSearch.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Diplay the top model parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Fold Mean Error Scores\n",
      "   Accuracy  Precision    Recall\n",
      "0  0.636642   0.623418  0.453569\n",
      "1  0.657036   0.659708  0.480608\n",
      "2  0.656696   0.652866  0.473806\n",
      "3  0.653977   0.655283  0.469060\n",
      "4  0.655676   0.670954  0.469970\n",
      "5  0.638001   0.640706  0.461826\n",
      "6  0.646159   0.651405  0.469970\n",
      "7  0.650238   0.637620  0.465839\n",
      "8  0.646159   0.634718  0.460587\n",
      "9  0.651258   0.649160  0.471756\n",
      "\n",
      "Average accuracy for all cv folds is: \t 0.64918\n",
      "Average precision for all cv folds is: \t 0.64758\n",
      "Average recall for all cv folds is: \t 0.4677\n",
      "*********************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Use the best parameters for our Linear Regression object\n",
    "OntimeClassifierEst = regGridSearch.best_estimator_\n",
    "\n",
    "bestScores = cross_validate(OntimeClassifierEst,X,y,scoring=['accuracy','precision','recall'], cv=cv_object, return_train_score=True)\n",
    "\n",
    "\n",
    "# grab the results from the dictionary into a dataframe\n",
    "cvScoreResult = pd.DataFrame()\n",
    "cvScoreResult['Accuracy'] = bestScores['test_accuracy']\n",
    "cvScoreResult['Precision'] = bestScores['test_precision']\n",
    "cvScoreResult['Recall'] = bestScores['test_recall']\n",
    "\n",
    "\n",
    "print('Cross Validation Fold Mean Error Scores')\n",
    "print(cvScoreResult)\n",
    "\n",
    "avgAccuracy = bestScores['test_accuracy'].mean()\n",
    "avgPrecision = bestScores['test_precision'].mean()\n",
    "avgRecall = bestScores['test_recall'].mean()\n",
    "\n",
    "\n",
    "avgAccStr = \"\\nAverage accuracy for all cv folds is: \\t {avgAccuracy:.5}\"\n",
    "avgPrcStr = \"Average precision for all cv folds is: \\t {avgPrecision:.5}\"\n",
    "avgRecStr = \"Average recall for all cv folds is: \\t {avgRecall:.5}\"\n",
    "\n",
    "print(avgAccStr.format(avgAccuracy=avgAccuracy))\n",
    "print(avgPrcStr.format(avgPrecision=avgPrecision))\n",
    "print(avgRecStr.format(avgRecall=avgRecall))\n",
    "print('*********************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run through the cross validation loop and set the training and testing variable for one single iteration \n",
    "#Code addapted from the Dataming Notbooks. Logistic Regression and SVM Notbook 4\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "#scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A case of a simple classification task, in which the two classes of points are well separated:\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X1, y1 = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=y1, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Linear Kernel (source: https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/)\n",
    "svclassifier = SVC(kernel='linear')  \n",
    "svclassifier.fit(X_train_scaled, y_train) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make predictions, the predict method of the SVC class is used. \n",
    "y_pred = svclassifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix, precision, recall, and F1 measures are the most commonly used metrics for classification tasks. \n",
    "#Scikit-Learn's metrics library contains the classification_report and confusion_matrix methods, which can be readily used to find out the values for these important metrics.\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using linear kernel to get the bar chart of coef weights\n",
    "print(svclassifier.coef_)\n",
    "weights = pd.Series(svclassifier.coef_[0],index=Accident_forLr.columns)\n",
    "weights.plot(kind='bar', figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], c=y1, s=50, cmap='autumn');\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a maximum margin estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMs on the data, Running the rbf model with scaled data to improve the accuracy \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )\n",
    "print(svm_clf.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tested_on = Accident_forLr.iloc[train_indices]\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "svm_clf.support_df = pd.DataFrame(svm_clf.support_)\n",
    "df_support['within 9 minutes NFPA standard'] = y[svm_clf.support_]# add back in the  Column to the pandas dataframe\n",
    "Accident_forLr['within 9 minutes NFPA standard'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now lets see the statistics of these attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "\n",
    "df_grouped_support = df_support.groupby(['within 9 minutes NFPA standard'])\n",
    "df_grouped = Accident_forLr.groupby(['within 9 minutes NFPA standard'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['route_County Road','route_Interstate','route_State Highway','atmos_Clear','light_Dark – Lighted',\n",
    "               'light_Dark – Not Lighted']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Perished','within 9 minutes NFPA standard'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Perished','within 9 minutes NFPA standard'])\n",
    "    plt.title(v+' (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the analysis here is basically telling us what the original statistics of the data looked like, and also what the statistics of the support vectors looked like. We can see that the separation in distributions is not as great as the separation for the original data. This is because the support vectors tend to be instances on the edge of the class boundaries and also instances that are classified incorrectly in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Weight Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the weights below, you can easily tell the direction of the relationship each dependent variable had. A negative value suggests that, if variable is true or increases, the likely hood of an on time arrival would increase as well.  We will need to scale these attributes to see the magnitude each one has on arrival time. \n",
    "\n",
    "Talk about parameters we can change to make model more acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "#Generates weights from the LR model.\n",
    "\n",
    "\n",
    "# iterate over the coefficients \n",
    "weights = OnTime_lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = Accident_forLr.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuses in meeting \n",
    "Weights changed to negative \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "OnTime_lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = OnTime_lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(OnTime_lr_clf.coef_.T,Accident_forLr.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights based on the Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#traffic might suck more than living in the country??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
