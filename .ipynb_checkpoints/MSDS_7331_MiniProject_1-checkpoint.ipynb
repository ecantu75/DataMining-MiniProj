{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 Mini - Project 1  \n",
    "\n",
    "## SVM and Logistic Modeling\n",
    "\n",
    "Professor: Dr. Jake Drew  \n",
    "Team: Steven Hayden, Josephine MacDaniel, Korey MacVittie, Afreen Siddiqui, Eduardo Cantu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp1\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "Accident_df_2016 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2016.csv',low_memory=False)\n",
    "Accident_df_2015 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2015.csv',low_memory=False)\n",
    "Accident_df = pd.concat([Accident_df_2015,Accident_df_2016])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66978 entries, 0 to 34438\n",
      "Data columns (total 71 columns):\n",
      "Unnamed: 0                                                     66978 non-null int64\n",
      "state_number                                                   66978 non-null int64\n",
      "state_name                                                     66978 non-null object\n",
      "consecutive_number                                             66978 non-null int64\n",
      "number_of_vehicle_forms_submitted_all                          66978 non-null int64\n",
      "number_of_motor_vehicles_in_transport_mvit                     66978 non-null int64\n",
      "number_of_parked_working_vehicles                              66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_not_in_motor_vehicles    66978 non-null int64\n",
      "number_of_persons_not_in_motor_vehicles_in_transport_mvit      66978 non-null int64\n",
      "number_of_persons_in_motor_vehicles_in_transport_mvit          66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_in_motor_vehicles        66978 non-null int64\n",
      "county                                                         66978 non-null int64\n",
      "city                                                           66978 non-null int64\n",
      "day_of_crash                                                   66978 non-null int64\n",
      "month_of_crash                                                 66978 non-null int64\n",
      "year_of_crash                                                  66978 non-null int64\n",
      "day_of_week                                                    66978 non-null int64\n",
      "hour_of_crash                                                  66978 non-null int64\n",
      "minute_of_crash                                                66978 non-null int64\n",
      "national_highway_system                                        66978 non-null int64\n",
      "land_use                                                       66978 non-null int64\n",
      "land_use_name                                                  66978 non-null object\n",
      "functional_system                                              66978 non-null int64\n",
      "functional_system_name                                         66978 non-null object\n",
      "ownership                                                      66978 non-null int64\n",
      "ownership_name                                                 66978 non-null object\n",
      "route_signing                                                  66978 non-null int64\n",
      "route_signing_name                                             66978 non-null object\n",
      "trafficway_identifier                                          66978 non-null object\n",
      "trafficway_identifier_2                                        17686 non-null object\n",
      "milepoint                                                      66978 non-null int64\n",
      "latitude                                                       66978 non-null float64\n",
      "longitude                                                      66978 non-null float64\n",
      "special_jurisdiction                                           66978 non-null int64\n",
      "special_jurisdiction_name                                      66978 non-null object\n",
      "first_harmful_event                                            66978 non-null int64\n",
      "first_harmful_event_name                                       66978 non-null object\n",
      "manner_of_collision                                            66978 non-null int64\n",
      "manner_of_collision_name                                       66978 non-null object\n",
      "relation_to_junction_within_interchange_area                   66978 non-null object\n",
      "relation_to_junction_specific_location                         66978 non-null int64\n",
      "relation_to_junction_specific_location_name                    66978 non-null object\n",
      "type_of_intersection                                           66978 non-null object\n",
      "work_zone                                                      66978 non-null object\n",
      "relation_to_trafficway                                         66978 non-null int64\n",
      "relation_to_trafficway_name                                    66978 non-null object\n",
      "light_condition                                                66978 non-null int64\n",
      "light_condition_name                                           66978 non-null object\n",
      "atmospheric_conditions_1                                       66978 non-null int64\n",
      "atmospheric_conditions_1_name                                  66978 non-null object\n",
      "atmospheric_conditions_2                                       66978 non-null int64\n",
      "atmospheric_conditions_2_name                                  66978 non-null object\n",
      "atmospheric_conditions                                         66978 non-null int64\n",
      "atmospheric_conditions_name                                    66978 non-null object\n",
      "school_bus_related                                             66978 non-null object\n",
      "rail_grade_crossing_identifier                                 66978 non-null object\n",
      "hour_of_notification                                           66978 non-null int64\n",
      "minute_of_notification                                         66978 non-null int64\n",
      "hour_of_arrival_at_scene                                       66978 non-null int64\n",
      "minute_of_arrival_at_scene                                     66978 non-null int64\n",
      "hour_of_ems_arrival_at_hospital                                66978 non-null int64\n",
      "minute_of_ems_arrival_at_hospital                              66978 non-null int64\n",
      "related_factors_crash_level_1                                  66978 non-null int64\n",
      "related_factors_crash_level_1_name                             66978 non-null object\n",
      "related_factors_crash_level_2                                  66978 non-null int64\n",
      "related_factors_crash_level_2_name                             66978 non-null object\n",
      "related_factors_crash_level_3                                  66978 non-null int64\n",
      "related_factors_crash_level_3_name                             66978 non-null object\n",
      "number_of_fatalities                                           66978 non-null int64\n",
      "number_of_drunk_drivers                                        66978 non-null int64\n",
      "timestamp_of_crash                                             66978 non-null object\n",
      "dtypes: float64(2), int64(44), object(25)\n",
      "memory usage: 36.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Displays the record count of non-null Values per attribute and their data type. \n",
    "Accident_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record. \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicate records and keeps the most recent record. We assume the most resent is the most accurate \n",
    "Accident_df = Accident_df.drop_duplicates(['consecutive_number'],keep = 'last')\n",
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record. \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19149 records were removed due to missing time data.\n"
     ]
    }
   ],
   "source": [
    "#Remove any recoreds without  time informaiton for arival and crash time\n",
    "count_no_rec= Accident_df[(Accident_df['hour_of_crash']>24) | (Accident_df['hour_of_arrival_at_scene']>24)]\n",
    "Accident_df = Accident_df[(Accident_df['hour_of_crash']<=24) & (Accident_df['hour_of_arrival_at_scene']<=24)]\n",
    "print(count_no_rec.consecutive_number.count(), 'records were removed due to missing time data.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_of_crash</th>\n",
       "      <th>minute_of_crash</th>\n",
       "      <th>Crash_Time</th>\n",
       "      <th>hour_of_arrival_at_scene</th>\n",
       "      <th>minute_of_arrival_at_scene</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Response_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>20:00:00</td>\n",
       "      <td>00:47:00</td>\n",
       "      <td>20:47:00</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>21:01:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>19:10:00</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:24:00</td>\n",
       "      <td>19:24:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:18:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:25:00</td>\n",
       "      <td>14:25:00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:23:00</td>\n",
       "      <td>23:23:00</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:29:00</td>\n",
       "      <td>23:29:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>16:01:00</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:12:00</td>\n",
       "      <td>16:12:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour_of_crash minute_of_crash Crash_Time hour_of_arrival_at_scene  \\\n",
       "196      20:00:00        00:47:00   20:47:00                 21:00:00   \n",
       "240      19:00:00        00:10:00   19:10:00                 19:00:00   \n",
       "274      14:00:00        00:18:00   14:18:00                 14:00:00   \n",
       "312      23:00:00        00:23:00   23:23:00                 23:00:00   \n",
       "959      16:00:00        00:01:00   16:01:00                 16:00:00   \n",
       "\n",
       "    minute_of_arrival_at_scene Arrival_Time  Response_Time  \n",
       "196                   00:01:00     21:01:00             14  \n",
       "240                   00:24:00     19:24:00             14  \n",
       "274                   00:25:00     14:25:00              7  \n",
       "312                   00:29:00     23:29:00              6  \n",
       "959                   00:12:00     16:12:00             11  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new Feature. This is the ratio between the number of fatalities and the people involved in the accident.\n",
    "Accident_df['Fatalities_ratio'] = Accident_df.number_of_fatalities/(Accident_df.number_of_persons_not_in_motor_vehicles_in_transport_mvit + Accident_df.number_of_persons_in_motor_vehicles_in_transport_mvit)\n",
    "\n",
    "#Converts hour and min to datetime type\n",
    "#crash\n",
    "Accident_df.hour_of_crash = pd.to_timedelta(Accident_df.hour_of_crash,unit ='h')\n",
    "Accident_df.minute_of_crash= pd.to_timedelta(Accident_df.minute_of_crash,unit ='m')\n",
    "#arrival\n",
    "Accident_df.hour_of_arrival_at_scene = pd.to_timedelta(Accident_df.hour_of_arrival_at_scene,unit ='h')\n",
    "Accident_df.minute_of_arrival_at_scene = pd.to_timedelta(Accident_df.minute_of_arrival_at_scene,unit ='m')\n",
    "\n",
    "#concatenates Hour and Minutes together \n",
    "Accident_df['Crash_Time'] = Accident_df['hour_of_crash'] + Accident_df['minute_of_crash'] \n",
    "Accident_df['Arrival_Time'] = Accident_df['hour_of_arrival_at_scene'] + Accident_df['minute_of_arrival_at_scene']\n",
    "#creates a response_time variable from the two fields above and converts to min\n",
    "Accident_df['Response_Time'] = Accident_df['Arrival_Time'] - Accident_df['Crash_Time']\n",
    "total_response_time_in_min = pd.DatetimeIndex(Accident_df['Response_Time'])\n",
    "Accident_df['Response_Time']= total_response_time_in_min.hour * 60 + total_response_time_in_min.minute\n",
    "\n",
    "\n",
    "#gut check of calculation \n",
    "Accident_df[['hour_of_crash','minute_of_crash','Crash_Time','hour_of_arrival_at_scene','minute_of_arrival_at_scene','Arrival_Time','Response_Time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The National Fire Protection Association's (NFPA) has established standard for response time and procedures for Emergency Medical Service (EMS) to adhere to. \n",
    "\n",
    "From EMSword.com \"The NFPA 1710 standard is based upon a combination of accepted practices and more than 30 years of study, research, testing and validation. Members of the 1710 committee that developed the standard include representatives from various fire agencies and the International Association of City/County Managers (ICMA).\"\n",
    "\n",
    "The NFPA 1710 standard allows for a one-minute call evaluation and preparation, four minutes for the arrival of a unit with first responder. For a situation that requires an advanced life support equipment like an ambulance, their standard is 8 minutes after the call preparation.  \n",
    "\n",
    "We chose to use their standards as threshold to determine if the paramedics got to the scene of the accident in time. This would be a binary response 0, for not arriving within 9 minutes of the accident and 1 for being within the 9 minutes. \n",
    "Source: https://www.emsworld.com/article/10324786/ems-response-time-standards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the binary variable 'within 9 minutes NFPA standard'\n",
    "Accident_df['within 9 minutes NFPA standard'] = np.where(Accident_df['Response_Time']<=9,1,0)\n",
    "#Accident_df[['Response_Time','within 9 minutes NFPA standard']].head(10) # Verify the binary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Interstate', 'U.S. Highway', 'State Highway',\n",
       "       'Local Street – Township', 'Local Street – Municipality',\n",
       "       'County Road', 'Other',\n",
       "       'Local Street – Frontage Road (Since 1994)', 'Unknown'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for columns that might not be needed\n",
    "#Accident_df.info()\n",
    "#list(Accident_df)\n",
    "Accident_df.route_signing_name.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_name',\n",
       " 'route_signing_name',\n",
       " 'atmospheric_conditions_name',\n",
       " 'within 9 minutes NFPA standard']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a copy of the original Data\n",
    "Accident_forLr=Accident_df.copy()\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "for col in ['Unnamed: 0',\n",
    "             'state_number',\n",
    "             'consecutive_number',\n",
    "             'county',\n",
    "             'city',\n",
    "             'day_of_crash',\n",
    "             'month_of_crash',\n",
    "             'year_of_crash',\n",
    "             'day_of_week',\n",
    "             'hour_of_crash',\n",
    "             'minute_of_crash',\n",
    "             'national_highway_system',\n",
    "             'land_use',\n",
    "             'land_use_name',\n",
    "             'functional_system',\n",
    "             'functional_system_name',\n",
    "             'ownership',\n",
    "             'ownership_name',\n",
    "             'route_signing',\n",
    "             'trafficway_identifier',\n",
    "             'trafficway_identifier_2',\n",
    "             'latitude',\n",
    "             'longitude',\n",
    "             'special_jurisdiction',\n",
    "             'special_jurisdiction_name',\n",
    "             'first_harmful_event',\n",
    "             'first_harmful_event_name',\n",
    "             'manner_of_collision',\n",
    "             'manner_of_collision_name',\n",
    "             'relation_to_junction_within_interchange_area',\n",
    "             'relation_to_junction_specific_location',\n",
    "             'relation_to_junction_specific_location_name',\n",
    "             'type_of_intersection',\n",
    "             'work_zone',\n",
    "             'relation_to_trafficway',\n",
    "             'relation_to_trafficway_name',\n",
    "             'light_condition',\n",
    "             'light_condition_name',\n",
    "             'atmospheric_conditions_1',\n",
    "             'atmospheric_conditions_1_name',\n",
    "             'atmospheric_conditions_2',\n",
    "             'atmospheric_conditions_2_name',\n",
    "             'atmospheric_conditions',\n",
    "             'school_bus_related',\n",
    "             'rail_grade_crossing_identifier',\n",
    "             'hour_of_notification',\n",
    "             'minute_of_notification',\n",
    "             'hour_of_arrival_at_scene',\n",
    "             'minute_of_arrival_at_scene',\n",
    "             'hour_of_ems_arrival_at_hospital',\n",
    "             'minute_of_ems_arrival_at_hospital',\n",
    "             'related_factors_crash_level_1',\n",
    "             'related_factors_crash_level_1_name',\n",
    "             'related_factors_crash_level_2',\n",
    "             'related_factors_crash_level_2_name',\n",
    "             'related_factors_crash_level_3',\n",
    "             'related_factors_crash_level_3_name',\n",
    "             'milepoint',\n",
    "             'number_of_parked_working_vehicles',\n",
    "             'number_of_forms_submitted_for_persons_not_in_motor_vehicles',\n",
    "             'number_of_persons_not_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_persons_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_forms_submitted_for_persons_in_motor_vehicles',\n",
    "             'timestamp_of_crash',\n",
    "             'number_of_fatalities',\n",
    "             'number_of_drunk_drivers',\n",
    "             'Fatalities_ratio',\n",
    "             'number_of_vehicle_forms_submitted_all',\n",
    "             'Crash_Time',\n",
    "             'Arrival_Time',\n",
    "             'Response_Time',\n",
    "             'number_of_motor_vehicles_in_transport_mvit']:\n",
    "                    if col in Accident_forLr:\n",
    "                        del Accident_forLr[col]\n",
    "# List the columns left in the df\n",
    "list(Accident_forLr)\n",
    "\n",
    "#Accident_forLr.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with an unknown informaiton in the route, atmospheric conditions and state name\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Not Reported')]\n",
    "Accident_forLr.count()\n",
    "\n",
    "# Rearrange colums\n",
    "Accident_forLr=Accident_forLr[['within 9 minutes NFPA standard','state_name','route_signing_name','atmospheric_conditions_name']]\n",
    "#list(Accident_forLr)  # Check for the correct column sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding of the categorical data \"state_name\"\n",
    "tmp_state_df = pd.get_dummies(Accident_forLr.state_name,prefix='state')\n",
    "#tmp_state_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"route_signing_name\"\n",
    "tmp_route_df = pd.get_dummies(Accident_forLr.route_signing_name,prefix='route')\n",
    "#tmp_route_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"atmospheric_conditions_name\"\n",
    "tmp_atmos_df = pd.get_dummies(Accident_forLr.atmospheric_conditions_name,prefix='atmos')\n",
    "#tmp_atmos_df.head(20)\n",
    "\n",
    "Accident_forLr = pd.concat((Accident_forLr,tmp_state_df,tmp_route_df,tmp_atmos_df),axis=1) # add back into the dataframe\n",
    "list(Accident_forLr)\n",
    "#delete the categorical variable columns\n",
    "del Accident_forLr['state_name']\n",
    "del Accident_forLr['route_signing_name']\n",
    "del Accident_forLr['atmospheric_conditions_name']\n",
    "\n",
    "#list(Accident_forLr) # Check for the last colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accident_forLr.count() # Count Records and show columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'within 9 minutes NFPA standard' in Accident_forLr:\n",
    "    y = Accident_forLr['within 9 minutes NFPA standard'].values # get the labels we want\n",
    "    del Accident_forLr['within 9 minutes NFPA standard'] # get rid of the class label\n",
    "    X = Accident_forLr.values # use everything else to predict!\n",
    "    \n",
    "        ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "        #    have converted them into simple matrices to use with scikit learn\n",
    "\n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 10\n",
    "num_instances = len(y)\n",
    "\n",
    "# Cross Validation Object\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(X)\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.6331521739130435\n",
      "confusion matrix\n",
      " [[1234  377]\n",
      " [ 703  630]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6273777173913043\n",
      "confusion matrix\n",
      " [[1224  395]\n",
      " [ 702  623]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6389266304347826\n",
      "confusion matrix\n",
      " [[1265  391]\n",
      " [ 672  616]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.6345108695652174\n",
      "confusion matrix\n",
      " [[1271  387]\n",
      " [ 689  597]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.6372282608695652\n",
      "confusion matrix\n",
      " [[1250  420]\n",
      " [ 648  626]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.6457201086956522\n",
      "confusion matrix\n",
      " [[1243  388]\n",
      " [ 655  658]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.6256793478260869\n",
      "confusion matrix\n",
      " [[1257  392]\n",
      " [ 710  585]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.6287364130434783\n",
      "confusion matrix\n",
      " [[1234  396]\n",
      " [ 697  617]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.6389266304347826\n",
      "confusion matrix\n",
      " [[1267  370]\n",
      " [ 693  614]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.6338315217391305\n",
      "confusion matrix\n",
      " [[1208  412]\n",
      " [ 666  658]]\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    OnTime_lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = OnTime_lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Weight Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the weights below, you can easily tell the direction of the relationship each dependent variable had. A negative value suggests that, if variable is true or increases, the likely hood of an on time arrival would increase as well.  We will need to scale these attributes to see the magnitude each one has on arrival time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_Alabama has weight of -0.6103098548907129\n",
      "state_Alaska has weight of 0.2429812028574808\n",
      "state_Arizona has weight of 0.3616756946565459\n",
      "state_Arkansas has weight of -0.5544348844109757\n",
      "state_California has weight of -0.1128401804239327\n",
      "state_Colorado has weight of 0.8146154451016403\n",
      "state_Connecticut has weight of 0.7765198042538625\n",
      "state_Delaware has weight of 0.44762474750788595\n",
      "state_District of Columbia has weight of 1.082217713836575\n",
      "state_Florida has weight of 0.7014457843626938\n",
      "state_Georgia has weight of 0.21000398293877784\n",
      "state_Hawaii has weight of 0.7196863200756445\n",
      "state_Idaho has weight of -0.17309659716991163\n",
      "state_Illinois has weight of -0.16069877216499687\n",
      "state_Iowa has weight of -0.25226270730258854\n",
      "state_Kansas has weight of -0.6149089336230285\n",
      "state_Kentucky has weight of -0.015898820880550293\n",
      "state_Louisiana has weight of -0.3784721554074977\n",
      "state_Maine has weight of -0.2561665013063117\n",
      "state_Maryland has weight of -0.10151383481328272\n",
      "state_Massachusetts has weight of 0.7142108111252659\n",
      "state_Michigan has weight of 0.6144079192667193\n",
      "state_Minnesota has weight of 0.3395726831065609\n",
      "state_Mississippi has weight of -0.056454813277046004\n",
      "state_Missouri has weight of -0.5959719762559404\n",
      "state_Montana has weight of -1.0507657271928992\n",
      "state_Nebraska has weight of 0.049116170610192605\n",
      "state_Nevada has weight of 0.7815797797521518\n",
      "state_New Hampshire has weight of -0.7292907331917181\n",
      "state_New Jersey has weight of -0.248059010928886\n",
      "state_New Mexico has weight of -0.2620383575362288\n",
      "state_New York has weight of 0.8134191292169091\n",
      "state_North Carolina has weight of 0.4354095327641863\n",
      "state_North Dakota has weight of -0.5469321253725861\n",
      "state_Ohio has weight of 0.12786097493478513\n",
      "state_Oklahoma has weight of -0.1706829250529786\n",
      "state_Oregon has weight of 0.2491306195828141\n",
      "state_Pennsylvania has weight of 0.5506379492778083\n",
      "state_Rhode Island has weight of 0.9007437593816681\n",
      "state_South Carolina has weight of -0.37541946852585195\n",
      "state_South Dakota has weight of -0.725777753335297\n",
      "state_Tennessee has weight of 0.11314471464320988\n",
      "state_Texas has weight of -0.33331771066743204\n",
      "state_Utah has weight of -0.09976168897870646\n",
      "state_Vermont has weight of -0.08557982827548284\n",
      "state_Virginia has weight of 0.11204235908928856\n",
      "state_Washington has weight of -0.5064588145783355\n",
      "state_West Virginia has weight of -0.7925954879365477\n",
      "state_Wisconsin has weight of -0.46887993262562316\n",
      "state_Wyoming has weight of -1.2851454237865763\n",
      "route_County Road has weight of -0.49716733714266065\n",
      "route_Interstate has weight of -0.5079658930984531\n",
      "route_Local Street – Frontage Road (Since 1994) has weight of 0.3433115871850478\n",
      "route_Local Street – Municipality has weight of 0.8350913802375892\n",
      "route_Local Street – Township has weight of -0.32393771472930705\n",
      "route_State Highway has weight of -0.22872191725704924\n",
      "route_U.S. Highway has weight of -0.02629802676538756\n",
      "atmos_Blowing Sand, Soil, Dirt has weight of -0.07792779565492748\n",
      "atmos_Blowing Snow has weight of 0.18224939438245052\n",
      "atmos_Clear has weight of 0.30052730536819566\n",
      "atmos_Cloudy has weight of 0.1517876678172525\n",
      "atmos_Fog, Smog, Smoke has weight of -0.2114627197284792\n",
      "atmos_Freezing Rain or Drizzle has weight of -0.1191182009853891\n",
      "atmos_Rain has weight of 0.19517084860170975\n",
      "atmos_Severe Crosswinds has weight of -0.2015241656903491\n",
      "atmos_Sleet, Hail has weight of -0.3964778194900228\n",
      "atmos_Snow has weight of -0.2289124362022199\n"
     ]
    }
   ],
   "source": [
    "# interpret the weights\n",
    "#negative value is the probability that t \n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = OnTime_lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = Accident_forLr.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuses in meeting \n",
    "Weights changed to negative \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayden\\AppData\\Local\\Continuum\\anaconda32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\shayden\\AppData\\Local\\Continuum\\anaconda32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\shayden\\AppData\\Local\\Continuum\\anaconda32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6334918478260869\n",
      "[[1209  411]\n",
      " [ 668  656]]\n",
      "state_Alabama has weight of -0.1476845026846974\n",
      "route_County Road has weight of -0.14424774903384566\n",
      "route_Interstate has weight of -0.13879975365491473\n",
      "state_Montana has weight of -0.12711004844860477\n",
      "state_Wyoming has weight of -0.12583068390639168\n",
      "state_Missouri has weight of -0.11234503299011979\n",
      "state_Arkansas has weight of -0.09667084235468293\n",
      "state_Texas has weight of -0.08759641788603978\n",
      "state_Louisiana has weight of -0.0767523784521268\n",
      "state_West Virginia has weight of -0.07361202956869611\n",
      "state_New Hampshire has weight of -0.07063715842038332\n",
      "state_Wisconsin has weight of -0.06910042094633811\n",
      "route_State Highway has weight of -0.06598224832146367\n",
      "state_Kansas has weight of -0.06004387104125303\n",
      "state_South Dakota has weight of -0.058631683934587495\n",
      "atmos_Snow has weight of -0.05293172116554769\n",
      "state_North Dakota has weight of -0.043591863185850635\n",
      "route_Local Street – Township has weight of -0.04328449798848302\n",
      "state_South Carolina has weight of -0.04225842025735606\n",
      "atmos_Fog, Smog, Smoke has weight of -0.03953416441982169\n",
      "state_New Mexico has weight of -0.036178344837692715\n",
      "atmos_Sleet, Hail has weight of -0.03254384752588334\n",
      "state_New Jersey has weight of -0.03239873248688586\n",
      "state_Oklahoma has weight of -0.0313911875109475\n",
      "state_Iowa has weight of -0.028462386386983975\n",
      "state_Maine has weight of -0.02572705443420579\n",
      "state_Washington has weight of -0.02227883192498254\n",
      "state_Idaho has weight of -0.02165773519377441\n",
      "atmos_Cloudy has weight of -0.017812697360135906\n",
      "atmos_Severe Crosswinds has weight of -0.01687620369963346\n",
      "state_Utah has weight of -0.01335432143785049\n",
      "atmos_Freezing Rain or Drizzle has weight of -0.011053543031844202\n",
      "state_Mississippi has weight of -0.008354507624065852\n",
      "atmos_Blowing Sand, Soil, Dirt has weight of -0.007605155716274416\n",
      "state_Illinois has weight of -0.006922707579578868\n",
      "state_Vermont has weight of -0.006211801348504605\n",
      "state_Kentucky has weight of -0.005577402577526562\n",
      "state_California has weight of -0.0050088485508880355\n",
      "state_Maryland has weight of -0.0041762823302552736\n",
      "atmos_Rain has weight of -0.0012625377283272709\n",
      "atmos_Blowing Snow has weight of 0.003176523117953299\n",
      "state_Nebraska has weight of 0.0035206364923053408\n",
      "state_Virginia has weight of 0.006598029827669278\n",
      "state_Alaska has weight of 0.01581095981374205\n",
      "state_Tennessee has weight of 0.01699060708243238\n",
      "route_U.S. Highway has weight of 0.025267784030398272\n",
      "state_Ohio has weight of 0.029686400163178448\n",
      "route_Local Street – Frontage Road (Since 1994) has weight of 0.03644094004113142\n",
      "state_Oregon has weight of 0.03737977582467488\n",
      "state_Delaware has weight of 0.03909282180915383\n",
      "state_Florida has weight of 0.04102209482145783\n",
      "state_Georgia has weight of 0.04255829124036442\n",
      "state_Colorado has weight of 0.04377911588906709\n",
      "atmos_Clear has weight of 0.04478521777892629\n",
      "state_Minnesota has weight of 0.04899240771362309\n",
      "state_Rhode Island has weight of 0.05777752978407728\n",
      "state_Massachusetts has weight of 0.060095703908673186\n",
      "state_Hawaii has weight of 0.060541323677242714\n",
      "state_District of Columbia has weight of 0.06261658688913059\n",
      "state_Arizona has weight of 0.06953421049575034\n",
      "state_Nevada has weight of 0.08596511906349331\n",
      "state_Connecticut has weight of 0.09538417627748562\n",
      "state_North Carolina has weight of 0.11166603139020484\n",
      "state_Pennsylvania has weight of 0.11291440722243298\n",
      "state_Michigan has weight of 0.11314842205735227\n",
      "state_New York has weight of 0.16649740739835436\n",
      "route_Local Street – Municipality has weight of 0.3376584255315201\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "OnTime_lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = OnTime_lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(OnTime_lr_clf.coef_.T,Accident_forLr.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights based on the Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#traffic might suck more than living in the country??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
