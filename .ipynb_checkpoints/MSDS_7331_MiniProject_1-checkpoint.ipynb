{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 Mini - Project 1  \n",
    "\n",
    "## SVM and Logistic Modeling\n",
    "\n",
    "Professor: Dr. Jake Drew  \n",
    "Team: Steven Hayden, Josephine MacDaniel, Korey MacVittie, Afreen Siddiqui, Eduardo Cantu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp1\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "Accident_df_2016 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2016.csv',low_memory=False)\n",
    "Accident_df_2015 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2015.csv',low_memory=False)\n",
    "Accident_df = pd.concat([Accident_df_2015,Accident_df_2016])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66978 entries, 0 to 34438\n",
      "Data columns (total 71 columns):\n",
      "Unnamed: 0                                                     66978 non-null int64\n",
      "state_number                                                   66978 non-null int64\n",
      "state_name                                                     66978 non-null object\n",
      "consecutive_number                                             66978 non-null int64\n",
      "number_of_vehicle_forms_submitted_all                          66978 non-null int64\n",
      "number_of_motor_vehicles_in_transport_mvit                     66978 non-null int64\n",
      "number_of_parked_working_vehicles                              66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_not_in_motor_vehicles    66978 non-null int64\n",
      "number_of_persons_not_in_motor_vehicles_in_transport_mvit      66978 non-null int64\n",
      "number_of_persons_in_motor_vehicles_in_transport_mvit          66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_in_motor_vehicles        66978 non-null int64\n",
      "county                                                         66978 non-null int64\n",
      "city                                                           66978 non-null int64\n",
      "day_of_crash                                                   66978 non-null int64\n",
      "month_of_crash                                                 66978 non-null int64\n",
      "year_of_crash                                                  66978 non-null int64\n",
      "day_of_week                                                    66978 non-null int64\n",
      "hour_of_crash                                                  66978 non-null int64\n",
      "minute_of_crash                                                66978 non-null int64\n",
      "national_highway_system                                        66978 non-null int64\n",
      "land_use                                                       66978 non-null int64\n",
      "land_use_name                                                  66978 non-null object\n",
      "functional_system                                              66978 non-null int64\n",
      "functional_system_name                                         66978 non-null object\n",
      "ownership                                                      66978 non-null int64\n",
      "ownership_name                                                 66978 non-null object\n",
      "route_signing                                                  66978 non-null int64\n",
      "route_signing_name                                             66978 non-null object\n",
      "trafficway_identifier                                          66978 non-null object\n",
      "trafficway_identifier_2                                        17686 non-null object\n",
      "milepoint                                                      66978 non-null int64\n",
      "latitude                                                       66978 non-null float64\n",
      "longitude                                                      66978 non-null float64\n",
      "special_jurisdiction                                           66978 non-null int64\n",
      "special_jurisdiction_name                                      66978 non-null object\n",
      "first_harmful_event                                            66978 non-null int64\n",
      "first_harmful_event_name                                       66978 non-null object\n",
      "manner_of_collision                                            66978 non-null int64\n",
      "manner_of_collision_name                                       66978 non-null object\n",
      "relation_to_junction_within_interchange_area                   66978 non-null object\n",
      "relation_to_junction_specific_location                         66978 non-null int64\n",
      "relation_to_junction_specific_location_name                    66978 non-null object\n",
      "type_of_intersection                                           66978 non-null object\n",
      "work_zone                                                      66978 non-null object\n",
      "relation_to_trafficway                                         66978 non-null int64\n",
      "relation_to_trafficway_name                                    66978 non-null object\n",
      "light_condition                                                66978 non-null int64\n",
      "light_condition_name                                           66978 non-null object\n",
      "atmospheric_conditions_1                                       66978 non-null int64\n",
      "atmospheric_conditions_1_name                                  66978 non-null object\n",
      "atmospheric_conditions_2                                       66978 non-null int64\n",
      "atmospheric_conditions_2_name                                  66978 non-null object\n",
      "atmospheric_conditions                                         66978 non-null int64\n",
      "atmospheric_conditions_name                                    66978 non-null object\n",
      "school_bus_related                                             66978 non-null object\n",
      "rail_grade_crossing_identifier                                 66978 non-null object\n",
      "hour_of_notification                                           66978 non-null int64\n",
      "minute_of_notification                                         66978 non-null int64\n",
      "hour_of_arrival_at_scene                                       66978 non-null int64\n",
      "minute_of_arrival_at_scene                                     66978 non-null int64\n",
      "hour_of_ems_arrival_at_hospital                                66978 non-null int64\n",
      "minute_of_ems_arrival_at_hospital                              66978 non-null int64\n",
      "related_factors_crash_level_1                                  66978 non-null int64\n",
      "related_factors_crash_level_1_name                             66978 non-null object\n",
      "related_factors_crash_level_2                                  66978 non-null int64\n",
      "related_factors_crash_level_2_name                             66978 non-null object\n",
      "related_factors_crash_level_3                                  66978 non-null int64\n",
      "related_factors_crash_level_3_name                             66978 non-null object\n",
      "number_of_fatalities                                           66978 non-null int64\n",
      "number_of_drunk_drivers                                        66978 non-null int64\n",
      "timestamp_of_crash                                             66978 non-null object\n",
      "dtypes: float64(2), int64(44), object(25)\n",
      "memory usage: 36.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Displays the record count of non-null Values per attribute and their data type. \n",
    "Accident_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record. \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicate records and keeps the most recent record. We assume the most resent is the most accurate \n",
    "Accident_df = Accident_df.drop_duplicates(['consecutive_number'],keep = 'last')\n",
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record. \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many accidents with missing data regarding the crash time and the arrival time of responders. This information is need for the dependent variable and in turn is crucial for our analysis. That is why it was decided to drop these records with missing data instead of filling the gaps with the mean.  The amount records dropped is about a third of the original data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19149 records were removed due to missing time data.\n"
     ]
    }
   ],
   "source": [
    "#Remove any recoreds without  time informaiton for arival and crash time\n",
    "count_no_rec= Accident_df[(Accident_df['hour_of_crash']>24) | (Accident_df['hour_of_arrival_at_scene']>24)]\n",
    "Accident_df = Accident_df[(Accident_df['hour_of_crash']<=24) & (Accident_df['hour_of_arrival_at_scene']<=24)]\n",
    "print(count_no_rec.consecutive_number.count(), 'records were removed due to missing time data.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_of_crash</th>\n",
       "      <th>minute_of_crash</th>\n",
       "      <th>Crash_Time</th>\n",
       "      <th>hour_of_arrival_at_scene</th>\n",
       "      <th>minute_of_arrival_at_scene</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Response_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>20:00:00</td>\n",
       "      <td>00:47:00</td>\n",
       "      <td>20:47:00</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>21:01:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>19:10:00</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:24:00</td>\n",
       "      <td>19:24:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:18:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:25:00</td>\n",
       "      <td>14:25:00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:23:00</td>\n",
       "      <td>23:23:00</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:29:00</td>\n",
       "      <td>23:29:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>16:01:00</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:12:00</td>\n",
       "      <td>16:12:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour_of_crash minute_of_crash Crash_Time hour_of_arrival_at_scene  \\\n",
       "196      20:00:00        00:47:00   20:47:00                 21:00:00   \n",
       "240      19:00:00        00:10:00   19:10:00                 19:00:00   \n",
       "274      14:00:00        00:18:00   14:18:00                 14:00:00   \n",
       "312      23:00:00        00:23:00   23:23:00                 23:00:00   \n",
       "959      16:00:00        00:01:00   16:01:00                 16:00:00   \n",
       "\n",
       "    minute_of_arrival_at_scene Arrival_Time  Response_Time  \n",
       "196                   00:01:00     21:01:00             14  \n",
       "240                   00:24:00     19:24:00             14  \n",
       "274                   00:25:00     14:25:00              7  \n",
       "312                   00:29:00     23:29:00              6  \n",
       "959                   00:12:00     16:12:00             11  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new Feature. This is the ratio between the number of fatalities and the people involved in the accident.\n",
    "Accident_df['Fatalities_ratio'] = Accident_df.number_of_fatalities/(Accident_df.number_of_persons_not_in_motor_vehicles_in_transport_mvit + Accident_df.number_of_persons_in_motor_vehicles_in_transport_mvit)\n",
    "\n",
    "#Converts hour and min to datetime type\n",
    "#crash\n",
    "Accident_df.hour_of_crash = pd.to_timedelta(Accident_df.hour_of_crash,unit ='h')\n",
    "Accident_df.minute_of_crash= pd.to_timedelta(Accident_df.minute_of_crash,unit ='m')\n",
    "#arrival\n",
    "Accident_df.hour_of_arrival_at_scene = pd.to_timedelta(Accident_df.hour_of_arrival_at_scene,unit ='h')\n",
    "Accident_df.minute_of_arrival_at_scene = pd.to_timedelta(Accident_df.minute_of_arrival_at_scene,unit ='m')\n",
    "\n",
    "#concatenates Hour and Minutes together \n",
    "Accident_df['Crash_Time'] = Accident_df['hour_of_crash'] + Accident_df['minute_of_crash'] \n",
    "Accident_df['Arrival_Time'] = Accident_df['hour_of_arrival_at_scene'] + Accident_df['minute_of_arrival_at_scene']\n",
    "#creates a response_time variable from the two fields above and converts to min\n",
    "Accident_df['Response_Time'] = Accident_df['Arrival_Time'] - Accident_df['Crash_Time']\n",
    "total_response_time_in_min = pd.DatetimeIndex(Accident_df['Response_Time'])\n",
    "Accident_df['Response_Time']= total_response_time_in_min.hour * 60 + total_response_time_in_min.minute\n",
    "\n",
    "\n",
    "#gut check of calculation \n",
    "Accident_df[['hour_of_crash','minute_of_crash','Crash_Time','hour_of_arrival_at_scene','minute_of_arrival_at_scene','Arrival_Time','Response_Time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The National Fire Protection Association's (NFPA) has established standard for response time and procedures for Emergency Medical Service (EMS) to adhere to. \n",
    "\n",
    "From EMSword.com \"The NFPA 1710 standard is based upon a combination of accepted practices and more than 30 years of study, research, testing and validation. Members of the 1710 committee that developed the standard include representatives from various fire agencies and the International Association of City/County Managers (ICMA).\"\n",
    "\n",
    "The NFPA 1710 standard allows for a one-minute call evaluation and preparation, four minutes for the arrival of a unit with first responder. For a situation that requires an advanced life support equipment like an ambulance, their standard is 8 minutes after the call preparation.  \n",
    "\n",
    "We chose to use their standards as threshold to determine if the paramedics got to the scene of the accident in time. This would be a binary response 0, for not arriving within 9 minutes of the accident and 1 for being within the 9 minutes. \n",
    "Source: https://www.emsworld.com/article/10324786/ems-response-time-standards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the binary variable 'within 9 minutes NFPA standard'\n",
    "Accident_df['within 9 minutes NFPA standard'] = np.where(Accident_df['Response_Time']<=9,1,0)\n",
    "#Accident_df[['Response_Time','within 9 minutes NFPA standard']].head(10) # Verify the binary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the logistic regresion will predict if the paramedic got on time to the scene of the accident we would select the attributes that we think affect this variable.\n",
    "\n",
    "First we would check what columns are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16230 entries, 196 to 34438\n",
      "Data columns (total 76 columns):\n",
      "Unnamed: 0                                                     16230 non-null int64\n",
      "state_number                                                   16230 non-null int64\n",
      "state_name                                                     16230 non-null object\n",
      "consecutive_number                                             16230 non-null int64\n",
      "number_of_vehicle_forms_submitted_all                          16230 non-null int64\n",
      "number_of_motor_vehicles_in_transport_mvit                     16230 non-null int64\n",
      "number_of_parked_working_vehicles                              16230 non-null int64\n",
      "number_of_forms_submitted_for_persons_not_in_motor_vehicles    16230 non-null int64\n",
      "number_of_persons_not_in_motor_vehicles_in_transport_mvit      16230 non-null int64\n",
      "number_of_persons_in_motor_vehicles_in_transport_mvit          16230 non-null int64\n",
      "number_of_forms_submitted_for_persons_in_motor_vehicles        16230 non-null int64\n",
      "county                                                         16230 non-null int64\n",
      "city                                                           16230 non-null int64\n",
      "day_of_crash                                                   16230 non-null int64\n",
      "month_of_crash                                                 16230 non-null int64\n",
      "year_of_crash                                                  16230 non-null int64\n",
      "day_of_week                                                    16230 non-null int64\n",
      "hour_of_crash                                                  16230 non-null timedelta64[ns]\n",
      "minute_of_crash                                                16230 non-null timedelta64[ns]\n",
      "national_highway_system                                        16230 non-null int64\n",
      "land_use                                                       16230 non-null int64\n",
      "land_use_name                                                  16230 non-null object\n",
      "functional_system                                              16230 non-null int64\n",
      "functional_system_name                                         16230 non-null object\n",
      "ownership                                                      16230 non-null int64\n",
      "ownership_name                                                 16230 non-null object\n",
      "route_signing                                                  16230 non-null int64\n",
      "route_signing_name                                             16230 non-null object\n",
      "trafficway_identifier                                          16230 non-null object\n",
      "trafficway_identifier_2                                        4119 non-null object\n",
      "milepoint                                                      16230 non-null int64\n",
      "latitude                                                       16230 non-null float64\n",
      "longitude                                                      16230 non-null float64\n",
      "special_jurisdiction                                           16230 non-null int64\n",
      "special_jurisdiction_name                                      16230 non-null object\n",
      "first_harmful_event                                            16230 non-null int64\n",
      "first_harmful_event_name                                       16230 non-null object\n",
      "manner_of_collision                                            16230 non-null int64\n",
      "manner_of_collision_name                                       16230 non-null object\n",
      "relation_to_junction_within_interchange_area                   16230 non-null object\n",
      "relation_to_junction_specific_location                         16230 non-null int64\n",
      "relation_to_junction_specific_location_name                    16230 non-null object\n",
      "type_of_intersection                                           16230 non-null object\n",
      "work_zone                                                      16230 non-null object\n",
      "relation_to_trafficway                                         16230 non-null int64\n",
      "relation_to_trafficway_name                                    16230 non-null object\n",
      "light_condition                                                16230 non-null int64\n",
      "light_condition_name                                           16230 non-null object\n",
      "atmospheric_conditions_1                                       16230 non-null int64\n",
      "atmospheric_conditions_1_name                                  16230 non-null object\n",
      "atmospheric_conditions_2                                       16230 non-null int64\n",
      "atmospheric_conditions_2_name                                  16230 non-null object\n",
      "atmospheric_conditions                                         16230 non-null int64\n",
      "atmospheric_conditions_name                                    16230 non-null object\n",
      "school_bus_related                                             16230 non-null object\n",
      "rail_grade_crossing_identifier                                 16230 non-null object\n",
      "hour_of_notification                                           16230 non-null int64\n",
      "minute_of_notification                                         16230 non-null int64\n",
      "hour_of_arrival_at_scene                                       16230 non-null timedelta64[ns]\n",
      "minute_of_arrival_at_scene                                     16230 non-null timedelta64[ns]\n",
      "hour_of_ems_arrival_at_hospital                                16230 non-null int64\n",
      "minute_of_ems_arrival_at_hospital                              16230 non-null int64\n",
      "related_factors_crash_level_1                                  16230 non-null int64\n",
      "related_factors_crash_level_1_name                             16230 non-null object\n",
      "related_factors_crash_level_2                                  16230 non-null int64\n",
      "related_factors_crash_level_2_name                             16230 non-null object\n",
      "related_factors_crash_level_3                                  16230 non-null int64\n",
      "related_factors_crash_level_3_name                             16230 non-null object\n",
      "number_of_fatalities                                           16230 non-null int64\n",
      "number_of_drunk_drivers                                        16230 non-null int64\n",
      "timestamp_of_crash                                             16230 non-null object\n",
      "Fatalities_ratio                                               16230 non-null float64\n",
      "Crash_Time                                                     16230 non-null timedelta64[ns]\n",
      "Arrival_Time                                                   16230 non-null timedelta64[ns]\n",
      "Response_Time                                                  16230 non-null int64\n",
      "within 9 minutes NFPA standard                                 16230 non-null int32\n",
      "dtypes: float64(3), int32(1), int64(41), object(25), timedelta64[ns](6)\n",
      "memory usage: 9.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#Displays all the available attributes on the dataset\n",
    "Accident_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the attributes in the dataset we would only select a subset to predict the if the paramedics will arrive on time or not.  \n",
    "* **state_name:** This attribute can give us an indication if there are states that would have a better response time in an accident than others.\n",
    "* **route_signing_name:** This can give us an insigth on the type of roads where the paramedics find more challenging to get.  \n",
    "* **light_condition_name:** We want to underdstand how much the lightning coditions affect the ability of the paramedics to get on time to the scene of the accident.  \n",
    "* **atmospheric_conditions_name:** Adding this attribute, we want to see how the weather conditions affect the reaction time of the paramedics.  \n",
    "* **within 9 minutes NFPA standard:**  This is our response variable, where 1 means that the paramedics get on time to the scene of the accident. As mentioned before, this threshold is set by the NFPA and is 9 minutes. This field will be 0 if the paramedics take more than 9 minutes to get the the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_name', 'route_signing_name', 'light_condition_name', 'atmospheric_conditions_name', 'within 9 minutes NFPA standard']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a copy of the original Data\n",
    "Accident_forLr=Accident_df.copy()\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "for col in ['Unnamed: 0',\n",
    "             'state_number',\n",
    "             'consecutive_number',\n",
    "             'county',\n",
    "             'city',\n",
    "             'day_of_crash',\n",
    "             'month_of_crash',\n",
    "             'year_of_crash',\n",
    "             'day_of_week',\n",
    "             'hour_of_crash',\n",
    "             'minute_of_crash',\n",
    "             'national_highway_system',\n",
    "             'land_use',\n",
    "             'land_use_name',\n",
    "             'functional_system',\n",
    "             'functional_system_name',\n",
    "             'ownership',\n",
    "             'ownership_name',\n",
    "             'route_signing',\n",
    "             'trafficway_identifier',\n",
    "             'trafficway_identifier_2',\n",
    "             'latitude',\n",
    "             'longitude',\n",
    "             'special_jurisdiction',\n",
    "             'special_jurisdiction_name',\n",
    "             'first_harmful_event',\n",
    "             'first_harmful_event_name',\n",
    "             'manner_of_collision',\n",
    "             'manner_of_collision_name',\n",
    "             'relation_to_junction_within_interchange_area',\n",
    "             'relation_to_junction_specific_location',\n",
    "             'relation_to_junction_specific_location_name',\n",
    "             'type_of_intersection',\n",
    "             'work_zone',\n",
    "             'relation_to_trafficway',\n",
    "             'relation_to_trafficway_name',\n",
    "             'light_condition',\n",
    "             'atmospheric_conditions_1',\n",
    "             'atmospheric_conditions_1_name',\n",
    "             'atmospheric_conditions_2',\n",
    "             'atmospheric_conditions_2_name',\n",
    "             'atmospheric_conditions',\n",
    "             'school_bus_related',\n",
    "             'rail_grade_crossing_identifier',\n",
    "             'hour_of_notification',\n",
    "             'minute_of_notification',\n",
    "             'hour_of_arrival_at_scene',\n",
    "             'minute_of_arrival_at_scene',\n",
    "             'hour_of_ems_arrival_at_hospital',\n",
    "             'minute_of_ems_arrival_at_hospital',\n",
    "             'related_factors_crash_level_1',\n",
    "             'related_factors_crash_level_1_name',\n",
    "             'related_factors_crash_level_2',\n",
    "             'related_factors_crash_level_2_name',\n",
    "             'related_factors_crash_level_3',\n",
    "             'related_factors_crash_level_3_name',\n",
    "             'milepoint',\n",
    "             'number_of_parked_working_vehicles',\n",
    "             'number_of_forms_submitted_for_persons_not_in_motor_vehicles',\n",
    "             'number_of_persons_not_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_persons_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_forms_submitted_for_persons_in_motor_vehicles',\n",
    "             'timestamp_of_crash',\n",
    "             'number_of_fatalities',\n",
    "             'number_of_drunk_drivers',\n",
    "             'Fatalities_ratio',\n",
    "             'number_of_vehicle_forms_submitted_all',\n",
    "             'Crash_Time',\n",
    "             'Arrival_Time',\n",
    "             'Response_Time',\n",
    "             'number_of_motor_vehicles_in_transport_mvit']:\n",
    "                    if col in Accident_forLr:\n",
    "                        del Accident_forLr[col]\n",
    "# List the columns left in the df\n",
    "list(Accident_forLr)\n",
    "\n",
    "#Accident_forLr.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any observation value on the selected attributes that is reported as *Unknown, Other, or Not Reported* would be removed from the dataset. This values do not bring any value when it comes to predict the response time of the paramedics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for 'route_signing_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Unknown' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Not Reported' 'Fog, Smog, Smoke' 'Severe Crosswinds'\n",
      " 'Freezing Rain or Drizzle' 'Blowing Sand, Soil, Dirt' 'Other'] \n",
      "\n",
      "Unique values for 'atmospheric_conditions_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Unknown' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Not Reported' 'Fog, Smog, Smoke' 'Severe Crosswinds'\n",
      " 'Freezing Rain or Drizzle' 'Blowing Sand, Soil, Dirt' 'Other'] \n",
      "\n",
      "Unique values for 'light_condition_name': ['Dark – Not Lighted' 'Daylight' 'Dark – Lighted' 'Dusk' 'Dawn'\n",
      " 'Dark – Unknown Lighting' 'Not Reported' 'Unknown' 'Other']\n"
     ]
    }
   ],
   "source": [
    "# Check for columns that might not be needed\n",
    "print(\"Unique values for 'route_signing_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'atmospheric_conditions_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'light_condition_name':\" , Accident_forLr.light_condition_name.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_name                        14706\n",
       "route_signing_name                14706\n",
       "light_condition_name              14706\n",
       "atmospheric_conditions_name       14706\n",
       "within 9 minutes NFPA standard    14706\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes observations with an unknown informaiton in the route, atmospheric conditions and state name\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Not Reported')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Not Reported')]\n",
    "Accident_forLr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review that the observations have been removed from the dataset. The undesired obsrvations are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for 'route_signing_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Fog, Smog, Smoke' 'Severe Crosswinds' 'Freezing Rain or Drizzle'\n",
      " 'Blowing Sand, Soil, Dirt'] \n",
      "\n",
      "Unique values for 'atmospheric_conditions_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Fog, Smog, Smoke' 'Severe Crosswinds' 'Freezing Rain or Drizzle'\n",
      " 'Blowing Sand, Soil, Dirt'] \n",
      "\n",
      "Unique values for 'light_condition_name': ['Dark – Not Lighted' 'Daylight' 'Dark – Lighted' 'Dusk' 'Dawn'\n",
      " 'Dark – Unknown Lighting']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values for 'route_signing_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'atmospheric_conditions_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'light_condition_name':\" , Accident_forLr.light_condition_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange colums\n",
    "Accident_forLr=Accident_forLr[['within 9 minutes NFPA standard','state_name','route_signing_name','atmospheric_conditions_name','light_condition_name']]\n",
    "#list(Accident_forLr)  # Check for the correct column sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below will perform the one-hot encoding of the variables on the dataset. This is to prepare the data in such a way that can be use for our logistic regresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'state_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-fa98aff07117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# perform one-hot encoding of the categorical data \"state_name\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtmp_state_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAccident_forLr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#tmp_state_df.head(20)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# perform one-hot encoding of the categorical data \"route_signing_name\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Masters\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'state_name'"
     ]
    }
   ],
   "source": [
    "# perform one-hot encoding of the categorical data \"state_name\"\n",
    "tmp_state_df = pd.get_dummies(Accident_forLr.state_name,prefix='state')\n",
    "#tmp_state_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"route_signing_name\"\n",
    "tmp_route_df = pd.get_dummies(Accident_forLr.route_signing_name,prefix='route')\n",
    "#tmp_route_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"atmospheric_conditions_name\"\n",
    "tmp_atmos_df = pd.get_dummies(Accident_forLr.atmospheric_conditions_name,prefix='atmos')\n",
    "#tmp_atmos_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"atmospheric_conditions_name\"\n",
    "tmp_light_df = pd.get_dummies(Accident_forLr.light_condition_name, prefix='light')\n",
    "#tmp_atmos_df.head(20)\n",
    "\n",
    "Accident_forLr = pd.concat((Accident_forLr,tmp_state_df,tmp_route_df,tmp_atmos_df,tmp_light_df), axis=1) # add back into the dataframe\n",
    "list(Accident_forLr)\n",
    "#delete the categorical variable columns\n",
    "del Accident_forLr['state_name']\n",
    "del Accident_forLr['route_signing_name']\n",
    "del Accident_forLr['atmospheric_conditions_name']\n",
    "del Accident_forLr['light_condition_name']\n",
    "\n",
    "#list(Accident_forLr) # Check for the last colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accident_forLr.count() # Count Records and show columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split and Cross Validation Setup\n",
    "In this logistic regresion we would be splitting our dataset into a Training and Test set. The split is going to be 80 % for training and the other 20 % for testing. A Cross Validation of ten fold would be performed to validate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'within 9 minutes NFPA standard' in Accident_forLr:\n",
    "    y = Accident_forLr['within 9 minutes NFPA standard'].values # get the labels we want\n",
    "    del Accident_forLr['within 9 minutes NFPA standard'] # get rid of the class label\n",
    "    X = Accident_forLr.values # use everything else to predict!\n",
    "    \n",
    "        ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "        #    have converted them into simple matrices to use with scikit learn\n",
    "\n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 10  #number of Cross Validation folds\n",
    "num_instances = len(y)\n",
    "\n",
    "# Cross Validation Object\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(X) # This prints all the dependant variables\n",
    "print(cv_object) # This print the Data split object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.6393609789259007\n",
      "confusion matrix\n",
      " [[1228  391]\n",
      " [ 670  653]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6390210740992522\n",
      "confusion matrix\n",
      " [[1252  359]\n",
      " [ 703  628]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6427600271923861\n",
      "confusion matrix\n",
      " [[1279  344]\n",
      " [ 707  612]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.6430999320190347\n",
      "confusion matrix\n",
      " [[1238  371]\n",
      " [ 679  654]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.6441196464989802\n",
      "confusion matrix\n",
      " [[1263  349]\n",
      " [ 698  632]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.6471787899388172\n",
      "confusion matrix\n",
      " [[1271  364]\n",
      " [ 674  633]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.6583956492182189\n",
      "confusion matrix\n",
      " [[1292  343]\n",
      " [ 662  645]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.6359619306594153\n",
      "confusion matrix\n",
      " [[1257  355]\n",
      " [ 716  614]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.6519374575118967\n",
      "confusion matrix\n",
      " [[1270  372]\n",
      " [ 652  648]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.6488783140720599\n",
      "confusion matrix\n",
      " [[1288  360]\n",
      " [ 673  621]]\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    OnTime_lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = OnTime_lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Weight Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the weights below, you can easily tell the direction of the relationship each dependent variable had. A negative value suggests that, if variable is true or increases, the likely hood of an on time arrival would increase as well.  We will need to scale these attributes to see the magnitude each one has on arrival time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "#negative value is the probability that t \n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = OnTime_lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = Accident_forLr.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuses in meeting \n",
    "Weights changed to negative \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "OnTime_lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = OnTime_lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(OnTime_lr_clf.coef_.T,Accident_forLr.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights based on the Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#traffic might suck more than living in the country??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
