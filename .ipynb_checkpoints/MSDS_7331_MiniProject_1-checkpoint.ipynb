{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 Mini - Project 1  \n",
    "\n",
    "## SVM and Logistic Modeling\n",
    "\n",
    "Professor: Dr. Jake Drew  \n",
    "Team: Steven Hayden, Josephine MacDaniel, Korey MacVittie, Afreen Siddiqui, Eduardo Cantu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mp1\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "Accident_df_2016 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2016.csv',low_memory=False)\n",
    "Accident_df_2015 = pd.read_csv('https://raw.githubusercontent.com/ecantu75/DataMining_Lab1/master/Data/accident_2015.csv',low_memory=False)\n",
    "Accident_df = pd.concat([Accident_df_2015,Accident_df_2016])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 66978 entries, 0 to 34438\n",
      "Data columns (total 71 columns):\n",
      "Unnamed: 0                                                     66978 non-null int64\n",
      "state_number                                                   66978 non-null int64\n",
      "state_name                                                     66978 non-null object\n",
      "consecutive_number                                             66978 non-null int64\n",
      "number_of_vehicle_forms_submitted_all                          66978 non-null int64\n",
      "number_of_motor_vehicles_in_transport_mvit                     66978 non-null int64\n",
      "number_of_parked_working_vehicles                              66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_not_in_motor_vehicles    66978 non-null int64\n",
      "number_of_persons_not_in_motor_vehicles_in_transport_mvit      66978 non-null int64\n",
      "number_of_persons_in_motor_vehicles_in_transport_mvit          66978 non-null int64\n",
      "number_of_forms_submitted_for_persons_in_motor_vehicles        66978 non-null int64\n",
      "county                                                         66978 non-null int64\n",
      "city                                                           66978 non-null int64\n",
      "day_of_crash                                                   66978 non-null int64\n",
      "month_of_crash                                                 66978 non-null int64\n",
      "year_of_crash                                                  66978 non-null int64\n",
      "day_of_week                                                    66978 non-null int64\n",
      "hour_of_crash                                                  66978 non-null int64\n",
      "minute_of_crash                                                66978 non-null int64\n",
      "national_highway_system                                        66978 non-null int64\n",
      "land_use                                                       66978 non-null int64\n",
      "land_use_name                                                  66978 non-null object\n",
      "functional_system                                              66978 non-null int64\n",
      "functional_system_name                                         66978 non-null object\n",
      "ownership                                                      66978 non-null int64\n",
      "ownership_name                                                 66978 non-null object\n",
      "route_signing                                                  66978 non-null int64\n",
      "route_signing_name                                             66978 non-null object\n",
      "trafficway_identifier                                          66978 non-null object\n",
      "trafficway_identifier_2                                        17686 non-null object\n",
      "milepoint                                                      66978 non-null int64\n",
      "latitude                                                       66978 non-null float64\n",
      "longitude                                                      66978 non-null float64\n",
      "special_jurisdiction                                           66978 non-null int64\n",
      "special_jurisdiction_name                                      66978 non-null object\n",
      "first_harmful_event                                            66978 non-null int64\n",
      "first_harmful_event_name                                       66978 non-null object\n",
      "manner_of_collision                                            66978 non-null int64\n",
      "manner_of_collision_name                                       66978 non-null object\n",
      "relation_to_junction_within_interchange_area                   66978 non-null object\n",
      "relation_to_junction_specific_location                         66978 non-null int64\n",
      "relation_to_junction_specific_location_name                    66978 non-null object\n",
      "type_of_intersection                                           66978 non-null object\n",
      "work_zone                                                      66978 non-null object\n",
      "relation_to_trafficway                                         66978 non-null int64\n",
      "relation_to_trafficway_name                                    66978 non-null object\n",
      "light_condition                                                66978 non-null int64\n",
      "light_condition_name                                           66978 non-null object\n",
      "atmospheric_conditions_1                                       66978 non-null int64\n",
      "atmospheric_conditions_1_name                                  66978 non-null object\n",
      "atmospheric_conditions_2                                       66978 non-null int64\n",
      "atmospheric_conditions_2_name                                  66978 non-null object\n",
      "atmospheric_conditions                                         66978 non-null int64\n",
      "atmospheric_conditions_name                                    66978 non-null object\n",
      "school_bus_related                                             66978 non-null object\n",
      "rail_grade_crossing_identifier                                 66978 non-null object\n",
      "hour_of_notification                                           66978 non-null int64\n",
      "minute_of_notification                                         66978 non-null int64\n",
      "hour_of_arrival_at_scene                                       66978 non-null int64\n",
      "minute_of_arrival_at_scene                                     66978 non-null int64\n",
      "hour_of_ems_arrival_at_hospital                                66978 non-null int64\n",
      "minute_of_ems_arrival_at_hospital                              66978 non-null int64\n",
      "related_factors_crash_level_1                                  66978 non-null int64\n",
      "related_factors_crash_level_1_name                             66978 non-null object\n",
      "related_factors_crash_level_2                                  66978 non-null int64\n",
      "related_factors_crash_level_2_name                             66978 non-null object\n",
      "related_factors_crash_level_3                                  66978 non-null int64\n",
      "related_factors_crash_level_3_name                             66978 non-null object\n",
      "number_of_fatalities                                           66978 non-null int64\n",
      "number_of_drunk_drivers                                        66978 non-null int64\n",
      "timestamp_of_crash                                             66978 non-null object\n",
      "dtypes: float64(2), int64(44), object(25)\n",
      "memory usage: 36.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Displays the record count of non-null Values per attribute and their data type. \n",
    "Accident_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record. \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicate records and keeps the most recent record. We assume the most resent is the most accurate \n",
    "Accident_df = Accident_df.drop_duplicates(['consecutive_number'],keep = 'last')\n",
    "\n",
    "#check for duplicate records. It displays the maximum count of a duplicated record. \n",
    "#Any value greater than 1 would mean that the data has duplicates\n",
    "Accident_df['consecutive_number'].value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many accidents with missing data regarding the crash time and the arrival time of responders. This information is need for the dependent variable and in turn is crucial for our analysis. That is why it was decided to drop these records with missing data instead of filling the gaps with the mean.  The amount records dropped is about a third of the original data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19149 records were removed due to missing time data.\n"
     ]
    }
   ],
   "source": [
    "#Remove any recoreds without  time informaiton for arival and crash time\n",
    "count_no_rec= Accident_df[(Accident_df['hour_of_crash']>24) | (Accident_df['hour_of_arrival_at_scene']>24)]\n",
    "Accident_df = Accident_df[(Accident_df['hour_of_crash']<=24) & (Accident_df['hour_of_arrival_at_scene']<=24)]\n",
    "print(count_no_rec.consecutive_number.count(), 'records were removed due to missing time data.') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_of_crash</th>\n",
       "      <th>minute_of_crash</th>\n",
       "      <th>Crash_Time</th>\n",
       "      <th>hour_of_arrival_at_scene</th>\n",
       "      <th>minute_of_arrival_at_scene</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Response_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>20:00:00</td>\n",
       "      <td>00:47:00</td>\n",
       "      <td>20:47:00</td>\n",
       "      <td>21:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>21:01:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>19:10:00</td>\n",
       "      <td>19:00:00</td>\n",
       "      <td>00:24:00</td>\n",
       "      <td>19:24:00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:18:00</td>\n",
       "      <td>14:18:00</td>\n",
       "      <td>14:00:00</td>\n",
       "      <td>00:25:00</td>\n",
       "      <td>14:25:00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:23:00</td>\n",
       "      <td>23:23:00</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>00:29:00</td>\n",
       "      <td>23:29:00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>16:01:00</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>00:12:00</td>\n",
       "      <td>16:12:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hour_of_crash minute_of_crash Crash_Time hour_of_arrival_at_scene  \\\n",
       "196      20:00:00        00:47:00   20:47:00                 21:00:00   \n",
       "240      19:00:00        00:10:00   19:10:00                 19:00:00   \n",
       "274      14:00:00        00:18:00   14:18:00                 14:00:00   \n",
       "312      23:00:00        00:23:00   23:23:00                 23:00:00   \n",
       "959      16:00:00        00:01:00   16:01:00                 16:00:00   \n",
       "\n",
       "    minute_of_arrival_at_scene Arrival_Time  Response_Time  \n",
       "196                   00:01:00     21:01:00             14  \n",
       "240                   00:24:00     19:24:00             14  \n",
       "274                   00:25:00     14:25:00              7  \n",
       "312                   00:29:00     23:29:00              6  \n",
       "959                   00:12:00     16:12:00             11  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new Feature. This is the ratio between the number of fatalities and the people involved in the accident.\n",
    "Accident_df['Fatalities_ratio'] = Accident_df.number_of_fatalities/(Accident_df.number_of_persons_not_in_motor_vehicles_in_transport_mvit + Accident_df.number_of_persons_in_motor_vehicles_in_transport_mvit)\n",
    "\n",
    "#Converts hour and min to datetime type\n",
    "#crash\n",
    "Accident_df.hour_of_crash = pd.to_timedelta(Accident_df.hour_of_crash,unit ='h')\n",
    "Accident_df.minute_of_crash= pd.to_timedelta(Accident_df.minute_of_crash,unit ='m')\n",
    "#arrival\n",
    "Accident_df.hour_of_arrival_at_scene = pd.to_timedelta(Accident_df.hour_of_arrival_at_scene,unit ='h')\n",
    "Accident_df.minute_of_arrival_at_scene = pd.to_timedelta(Accident_df.minute_of_arrival_at_scene,unit ='m')\n",
    "\n",
    "#concatenates Hour and Minutes together \n",
    "Accident_df['Crash_Time'] = Accident_df['hour_of_crash'] + Accident_df['minute_of_crash'] \n",
    "Accident_df['Arrival_Time'] = Accident_df['hour_of_arrival_at_scene'] + Accident_df['minute_of_arrival_at_scene']\n",
    "#creates a response_time variable from the two fields above and converts to min\n",
    "Accident_df['Response_Time'] = Accident_df['Arrival_Time'] - Accident_df['Crash_Time']\n",
    "total_response_time_in_min = pd.DatetimeIndex(Accident_df['Response_Time'])\n",
    "Accident_df['Response_Time']= total_response_time_in_min.hour * 60 + total_response_time_in_min.minute\n",
    "\n",
    "\n",
    "#gut check of calculation \n",
    "Accident_df[['hour_of_crash','minute_of_crash','Crash_Time','hour_of_arrival_at_scene','minute_of_arrival_at_scene','Arrival_Time','Response_Time']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The National Fire Protection Association's (NFPA) has established standard for response time and procedures for Emergency Medical Service (EMS) to adhere to. \n",
    "\n",
    "From EMSword.com \"The NFPA 1710 standard is based upon a combination of accepted practices and more than 30 years of study, research, testing and validation. Members of the 1710 committee that developed the standard include representatives from various fire agencies and the International Association of City/County Managers (ICMA).\"\n",
    "\n",
    "The NFPA 1710 standard allows for a one-minute call evaluation and preparation, four minutes for the arrival of a unit with first responder. For a situation that requires an advanced life support equipment like an ambulance, their standard is 8 minutes after the call preparation.  \n",
    "\n",
    "We chose to use their standards as threshold to determine if the paramedics got to the scene of the accident in time. This would be a binary response 0, for not arriving within 9 minutes of the accident and 1 for being within the 9 minutes. \n",
    "Source: https://www.emsworld.com/article/10324786/ems-response-time-standards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the binary variable 'within 9 minutes NFPA standard'\n",
    "Accident_df['within 9 minutes NFPA standard'] = np.where(Accident_df['Response_Time']<=9,1,0)\n",
    "#Accident_df[['Response_Time','within 9 minutes NFPA standard']].head(10) # Verify the binary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the logistic regresion will predict if the paramedic got on time to the scene of the accident we would select the attributes that we think affect this variable.\n",
    "\n",
    "First we would check what columns are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16230 entries, 196 to 34438\n",
      "Data columns (total 76 columns):\n",
      "Unnamed: 0                                                     16230 non-null int64\n",
      "state_number                                                   16230 non-null int64\n",
      "state_name                                                     16230 non-null object\n",
      "consecutive_number                                             16230 non-null int64\n",
      "number_of_vehicle_forms_submitted_all                          16230 non-null int64\n",
      "number_of_motor_vehicles_in_transport_mvit                     16230 non-null int64\n",
      "number_of_parked_working_vehicles                              16230 non-null int64\n",
      "number_of_forms_submitted_for_persons_not_in_motor_vehicles    16230 non-null int64\n",
      "number_of_persons_not_in_motor_vehicles_in_transport_mvit      16230 non-null int64\n",
      "number_of_persons_in_motor_vehicles_in_transport_mvit          16230 non-null int64\n",
      "number_of_forms_submitted_for_persons_in_motor_vehicles        16230 non-null int64\n",
      "county                                                         16230 non-null int64\n",
      "city                                                           16230 non-null int64\n",
      "day_of_crash                                                   16230 non-null int64\n",
      "month_of_crash                                                 16230 non-null int64\n",
      "year_of_crash                                                  16230 non-null int64\n",
      "day_of_week                                                    16230 non-null int64\n",
      "hour_of_crash                                                  16230 non-null timedelta64[ns]\n",
      "minute_of_crash                                                16230 non-null timedelta64[ns]\n",
      "national_highway_system                                        16230 non-null int64\n",
      "land_use                                                       16230 non-null int64\n",
      "land_use_name                                                  16230 non-null object\n",
      "functional_system                                              16230 non-null int64\n",
      "functional_system_name                                         16230 non-null object\n",
      "ownership                                                      16230 non-null int64\n",
      "ownership_name                                                 16230 non-null object\n",
      "route_signing                                                  16230 non-null int64\n",
      "route_signing_name                                             16230 non-null object\n",
      "trafficway_identifier                                          16230 non-null object\n",
      "trafficway_identifier_2                                        4119 non-null object\n",
      "milepoint                                                      16230 non-null int64\n",
      "latitude                                                       16230 non-null float64\n",
      "longitude                                                      16230 non-null float64\n",
      "special_jurisdiction                                           16230 non-null int64\n",
      "special_jurisdiction_name                                      16230 non-null object\n",
      "first_harmful_event                                            16230 non-null int64\n",
      "first_harmful_event_name                                       16230 non-null object\n",
      "manner_of_collision                                            16230 non-null int64\n",
      "manner_of_collision_name                                       16230 non-null object\n",
      "relation_to_junction_within_interchange_area                   16230 non-null object\n",
      "relation_to_junction_specific_location                         16230 non-null int64\n",
      "relation_to_junction_specific_location_name                    16230 non-null object\n",
      "type_of_intersection                                           16230 non-null object\n",
      "work_zone                                                      16230 non-null object\n",
      "relation_to_trafficway                                         16230 non-null int64\n",
      "relation_to_trafficway_name                                    16230 non-null object\n",
      "light_condition                                                16230 non-null int64\n",
      "light_condition_name                                           16230 non-null object\n",
      "atmospheric_conditions_1                                       16230 non-null int64\n",
      "atmospheric_conditions_1_name                                  16230 non-null object\n",
      "atmospheric_conditions_2                                       16230 non-null int64\n",
      "atmospheric_conditions_2_name                                  16230 non-null object\n",
      "atmospheric_conditions                                         16230 non-null int64\n",
      "atmospheric_conditions_name                                    16230 non-null object\n",
      "school_bus_related                                             16230 non-null object\n",
      "rail_grade_crossing_identifier                                 16230 non-null object\n",
      "hour_of_notification                                           16230 non-null int64\n",
      "minute_of_notification                                         16230 non-null int64\n",
      "hour_of_arrival_at_scene                                       16230 non-null timedelta64[ns]\n",
      "minute_of_arrival_at_scene                                     16230 non-null timedelta64[ns]\n",
      "hour_of_ems_arrival_at_hospital                                16230 non-null int64\n",
      "minute_of_ems_arrival_at_hospital                              16230 non-null int64\n",
      "related_factors_crash_level_1                                  16230 non-null int64\n",
      "related_factors_crash_level_1_name                             16230 non-null object\n",
      "related_factors_crash_level_2                                  16230 non-null int64\n",
      "related_factors_crash_level_2_name                             16230 non-null object\n",
      "related_factors_crash_level_3                                  16230 non-null int64\n",
      "related_factors_crash_level_3_name                             16230 non-null object\n",
      "number_of_fatalities                                           16230 non-null int64\n",
      "number_of_drunk_drivers                                        16230 non-null int64\n",
      "timestamp_of_crash                                             16230 non-null object\n",
      "Fatalities_ratio                                               16230 non-null float64\n",
      "Crash_Time                                                     16230 non-null timedelta64[ns]\n",
      "Arrival_Time                                                   16230 non-null timedelta64[ns]\n",
      "Response_Time                                                  16230 non-null int64\n",
      "within 9 minutes NFPA standard                                 16230 non-null int32\n",
      "dtypes: float64(3), int32(1), int64(41), object(25), timedelta64[ns](6)\n",
      "memory usage: 9.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#Displays all the available attributes on the dataset\n",
    "Accident_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the attributes in the dataset we would only select a subset to predict the if the paramedics will arrive on time or not.  \n",
    "* **state_name:** This attribute can give us an indication if there are states that would have a better response time in an accident than others.\n",
    "* **route_signing_name:** This can give us an insigth on the type of roads where the paramedics find more challenging to get.  \n",
    "* **light_condition_name:** We want to underdstand how much the lightning coditions affect the ability of the paramedics to get on time to the scene of the accident.  \n",
    "* **atmospheric_conditions_name:** Adding this attribute, we want to see how the weather conditions affect the reaction time of the paramedics.  \n",
    "* **within 9 minutes NFPA standard:**  This is our response variable, where 1 means that the paramedics get on time to the scene of the accident. As mentioned before, this threshold is set by the NFPA and is 9 minutes. This field will be 0 if the paramedics take more than 9 minutes to get the the accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_name',\n",
       " 'route_signing_name',\n",
       " 'light_condition_name',\n",
       " 'atmospheric_conditions_name',\n",
       " 'within 9 minutes NFPA standard']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a copy of the original Data\n",
    "Accident_forLr=Accident_df.copy()\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "for col in ['Unnamed: 0',\n",
    "             'state_number',\n",
    "             'consecutive_number',\n",
    "             'county',\n",
    "             'city',\n",
    "             'day_of_crash',\n",
    "             'month_of_crash',\n",
    "             'year_of_crash',\n",
    "             'day_of_week',\n",
    "             'hour_of_crash',\n",
    "             'minute_of_crash',\n",
    "             'national_highway_system',\n",
    "             'land_use',\n",
    "             'land_use_name',\n",
    "             'functional_system',\n",
    "             'functional_system_name',\n",
    "             'ownership',\n",
    "             'ownership_name',\n",
    "             'route_signing',\n",
    "             'trafficway_identifier',\n",
    "             'trafficway_identifier_2',\n",
    "             'latitude',\n",
    "             'longitude',\n",
    "             'special_jurisdiction',\n",
    "             'special_jurisdiction_name',\n",
    "             'first_harmful_event',\n",
    "             'first_harmful_event_name',\n",
    "             'manner_of_collision',\n",
    "             'manner_of_collision_name',\n",
    "             'relation_to_junction_within_interchange_area',\n",
    "             'relation_to_junction_specific_location',\n",
    "             'relation_to_junction_specific_location_name',\n",
    "             'type_of_intersection',\n",
    "             'work_zone',\n",
    "             'relation_to_trafficway',\n",
    "             'relation_to_trafficway_name',\n",
    "             'light_condition',\n",
    "             'atmospheric_conditions_1',\n",
    "             'atmospheric_conditions_1_name',\n",
    "             'atmospheric_conditions_2',\n",
    "             'atmospheric_conditions_2_name',\n",
    "             'atmospheric_conditions',\n",
    "             'school_bus_related',\n",
    "             'rail_grade_crossing_identifier',\n",
    "             'hour_of_notification',\n",
    "             'minute_of_notification',\n",
    "             'hour_of_arrival_at_scene',\n",
    "             'minute_of_arrival_at_scene',\n",
    "             'hour_of_ems_arrival_at_hospital',\n",
    "             'minute_of_ems_arrival_at_hospital',\n",
    "             'related_factors_crash_level_1',\n",
    "             'related_factors_crash_level_1_name',\n",
    "             'related_factors_crash_level_2',\n",
    "             'related_factors_crash_level_2_name',\n",
    "             'related_factors_crash_level_3',\n",
    "             'related_factors_crash_level_3_name',\n",
    "             'milepoint',\n",
    "             'number_of_parked_working_vehicles',\n",
    "             'number_of_forms_submitted_for_persons_not_in_motor_vehicles',\n",
    "             'number_of_persons_not_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_persons_in_motor_vehicles_in_transport_mvit',\n",
    "             'number_of_forms_submitted_for_persons_in_motor_vehicles',\n",
    "             'timestamp_of_crash',\n",
    "             'number_of_fatalities',\n",
    "             'number_of_drunk_drivers',\n",
    "             'Fatalities_ratio',\n",
    "             'number_of_vehicle_forms_submitted_all',\n",
    "             'Crash_Time',\n",
    "             'Arrival_Time',\n",
    "             'Response_Time',\n",
    "             'number_of_motor_vehicles_in_transport_mvit']:\n",
    "                    if col in Accident_forLr:\n",
    "                        del Accident_forLr[col]\n",
    "# List the columns left in the df\n",
    "list(Accident_forLr)\n",
    "\n",
    "#Accident_forLr.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any observation value on the selected attributes that is reported as *Unknown, Other, or Not Reported* would be removed from the dataset. This values do not bring any value when it comes to predict the response time of the paramedics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for 'route_signing_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Unknown' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Not Reported' 'Fog, Smog, Smoke' 'Severe Crosswinds'\n",
      " 'Freezing Rain or Drizzle' 'Blowing Sand, Soil, Dirt' 'Other'] \n",
      "\n",
      "Unique values for 'atmospheric_conditions_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Unknown' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Not Reported' 'Fog, Smog, Smoke' 'Severe Crosswinds'\n",
      " 'Freezing Rain or Drizzle' 'Blowing Sand, Soil, Dirt' 'Other'] \n",
      "\n",
      "Unique values for 'light_condition_name': ['Dark – Not Lighted' 'Daylight' 'Dark – Lighted' 'Dusk' 'Dawn'\n",
      " 'Dark – Unknown Lighting' 'Not Reported' 'Unknown' 'Other']\n"
     ]
    }
   ],
   "source": [
    "# Check for columns that might not be needed\n",
    "print(\"Unique values for 'route_signing_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'atmospheric_conditions_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'light_condition_name':\" , Accident_forLr.light_condition_name.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_name                        14706\n",
       "route_signing_name                14706\n",
       "light_condition_name              14706\n",
       "atmospheric_conditions_name       14706\n",
       "within 9 minutes NFPA standard    14706\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removes observations with an unknown informaiton in the route, atmospheric conditions and state name\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['route_signing_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['atmospheric_conditions_name'] != 'Not Reported')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Unknown')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Other')]\n",
    "Accident_forLr= Accident_forLr[(Accident_forLr['light_condition_name'] != 'Not Reported')]\n",
    "Accident_forLr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review that the observations have been removed from the dataset. The undesired obsrvations are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values for 'route_signing_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Fog, Smog, Smoke' 'Severe Crosswinds' 'Freezing Rain or Drizzle'\n",
      " 'Blowing Sand, Soil, Dirt'] \n",
      "\n",
      "Unique values for 'atmospheric_conditions_name': ['Rain' 'Snow' 'Clear' 'Cloudy' 'Sleet, Hail' 'Blowing Snow'\n",
      " 'Fog, Smog, Smoke' 'Severe Crosswinds' 'Freezing Rain or Drizzle'\n",
      " 'Blowing Sand, Soil, Dirt'] \n",
      "\n",
      "Unique values for 'light_condition_name': ['Dark – Not Lighted' 'Daylight' 'Dark – Lighted' 'Dusk' 'Dawn'\n",
      " 'Dark – Unknown Lighting']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values for 'route_signing_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'atmospheric_conditions_name':\" , Accident_forLr.atmospheric_conditions_name.unique(), \"\\n\")\n",
    "print(\"Unique values for 'light_condition_name':\" , Accident_forLr.light_condition_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange colums\n",
    "Accident_forLr=Accident_forLr[['within 9 minutes NFPA standard','state_name','route_signing_name','atmospheric_conditions_name','light_condition_name']]\n",
    "#list(Accident_forLr)  # Check for the correct column sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section below will perform the one-hot encoding of the variables on the dataset. This is to prepare the data in such a way that can be use for our logistic regresion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-hot encoding of the categorical data \"state_name\"\n",
    "tmp_state_df = pd.get_dummies(Accident_forLr.state_name,prefix='state')\n",
    "#tmp_state_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"route_signing_name\"\n",
    "tmp_route_df = pd.get_dummies(Accident_forLr.route_signing_name,prefix='route')\n",
    "#tmp_route_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"atmospheric_conditions_name\"\n",
    "tmp_atmos_df = pd.get_dummies(Accident_forLr.atmospheric_conditions_name,prefix='atmos')\n",
    "#tmp_atmos_df.head(20)\n",
    "\n",
    "# perform one-hot encoding of the categorical data \"atmospheric_conditions_name\"\n",
    "tmp_light_df = pd.get_dummies(Accident_forLr.light_condition_name, prefix='light')\n",
    "#tmp_atmos_df.head(20)\n",
    "\n",
    "Accident_forLr = pd.concat((Accident_forLr,tmp_state_df,tmp_route_df,tmp_atmos_df,tmp_light_df), axis=1) # add back into the dataframe\n",
    "list(Accident_forLr)\n",
    "#delete the categorical variable columns\n",
    "del Accident_forLr['state_name']\n",
    "del Accident_forLr['route_signing_name']\n",
    "del Accident_forLr['atmospheric_conditions_name']\n",
    "del Accident_forLr['light_condition_name']\n",
    "\n",
    "#list(Accident_forLr) # Check for the last colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accident_forLr.count() # Count Records and show columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split and Cross Validation Setup\n",
    "In this logistic regresion we would be splitting our dataset into a Training and Test set. The split is going to be 80 % for training and the other 20 % for testing. The ratio used is accepted split in the research community, and we did not see any reason to deviate from this ratio. A Cross Validation of ten fold would be performed to validate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "ShuffleSplit(n_splits=10, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'within 9 minutes NFPA standard' in Accident_forLr:\n",
    "    y = Accident_forLr['within 9 minutes NFPA standard'].values # get the labels we want\n",
    "    del Accident_forLr['within 9 minutes NFPA standard'] # get rid of the class label\n",
    "    X = Accident_forLr.values # use everything else to predict!\n",
    "    \n",
    "        ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "        #    have converted them into simple matrices to use with scikit learn\n",
    "\n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 10  #number of Cross Validation folds\n",
    "num_instances = len(y)\n",
    "\n",
    "# Cross Validation Object\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(X) # This prints all the dependant variables\n",
    "print(cv_object) # This print the Data split object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.6437797416723318\n",
      "confusion matrix\n",
      " [[1260  382]\n",
      " [ 666  634]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.6335825968728755\n",
      "confusion matrix\n",
      " [[1252  356]\n",
      " [ 722  612]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.6587355540448674\n",
      "confusion matrix\n",
      " [[1272  365]\n",
      " [ 639  666]]\n",
      "====Iteration 3  ====\n",
      "accuracy 0.6447994561522774\n",
      "confusion matrix\n",
      " [[1276  350]\n",
      " [ 695  621]]\n",
      "====Iteration 4  ====\n",
      "accuracy 0.6383412644459552\n",
      "confusion matrix\n",
      " [[1244  388]\n",
      " [ 676  634]]\n",
      "====Iteration 5  ====\n",
      "accuracy 0.6451393609789259\n",
      "confusion matrix\n",
      " [[1272  359]\n",
      " [ 685  626]]\n",
      "====Iteration 6  ====\n",
      "accuracy 0.6495581237253569\n",
      "confusion matrix\n",
      " [[1300  329]\n",
      " [ 702  611]]\n",
      "====Iteration 7  ====\n",
      "accuracy 0.6390210740992522\n",
      "confusion matrix\n",
      " [[1258  363]\n",
      " [ 699  622]]\n",
      "====Iteration 8  ====\n",
      "accuracy 0.6352821210061182\n",
      "confusion matrix\n",
      " [[1248  350]\n",
      " [ 723  621]]\n",
      "====Iteration 9  ====\n",
      "accuracy 0.6393609789259007\n",
      "confusion matrix\n",
      " [[1232  382]\n",
      " [ 679  649]]\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " ==========================\n",
      "average accuracy 0.6427600271923861\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "acc_List = []\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    OnTime_lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = OnTime_lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    acc_List.append(acc)\n",
    "    iter_num+=1\n",
    "\n",
    "    \n",
    "# The Code above randomly creates a new training and testing set\n",
    "# so there will multiple accuracy measurements\n",
    "# We have taken the of all these measurements below\n",
    "\n",
    "print('\\n','\\n','\\n','\\n',\"==========================\") \n",
    "print(\"average accuracy\",np.mean(acc_List))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The mean of the 10-fold cross validation is indicated above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Weight Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the weights below, you can easily tell the direction of the relationship each dependent variable had. A negative value suggests that, if variable is true or increases, the likely hood of an on time arrival would increase as well.  We will need to scale these attributes to see the magnitude each one has on arrival time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_Alabama has weight of -0.4899829837392541\n",
      "state_Alaska has weight of -0.14337924768566845\n",
      "state_Arizona has weight of 0.3232682314219525\n",
      "state_Arkansas has weight of -0.45203186632788106\n",
      "state_California has weight of -0.21335616389767512\n",
      "state_Colorado has weight of 0.6327633153712515\n",
      "state_Connecticut has weight of 0.8458498665882528\n",
      "state_Delaware has weight of 0.9540642307433653\n",
      "state_District of Columbia has weight of 0.5062271668474364\n",
      "state_Florida has weight of 0.5612335530299186\n",
      "state_Georgia has weight of 0.24518265462769034\n",
      "state_Hawaii has weight of 0.4356375585345174\n",
      "state_Idaho has weight of -0.11161754368952305\n",
      "state_Illinois has weight of 0.38979241292011957\n",
      "state_Iowa has weight of -0.12233878747115826\n",
      "state_Kansas has weight of -0.6562818149204045\n",
      "state_Kentucky has weight of 0.08049679931043642\n",
      "state_Louisiana has weight of -0.330356054142514\n",
      "state_Maine has weight of -0.013601800827420287\n",
      "state_Maryland has weight of -0.004566366469773999\n",
      "state_Massachusetts has weight of 0.6611151074392592\n",
      "state_Michigan has weight of 0.5230901659029853\n",
      "state_Minnesota has weight of 0.3905928690326433\n",
      "state_Mississippi has weight of 0.11215640959794526\n",
      "state_Missouri has weight of -0.550815464022322\n",
      "state_Montana has weight of -0.9864594650595493\n",
      "state_Nebraska has weight of -0.08079387797978817\n",
      "state_Nevada has weight of 0.5396778706659681\n",
      "state_New Hampshire has weight of -0.7122450471332069\n",
      "state_New Jersey has weight of -0.12971934841067687\n",
      "state_New Mexico has weight of -0.278657940329367\n",
      "state_New York has weight of 0.806173084088319\n",
      "state_North Carolina has weight of 0.5381997977507149\n",
      "state_North Dakota has weight of -0.8533970205633419\n",
      "state_Ohio has weight of 0.11176052168001094\n",
      "state_Oklahoma has weight of -0.18570565125462343\n",
      "state_Oregon has weight of 0.4153835949839596\n",
      "state_Pennsylvania has weight of 0.6508745009758508\n",
      "state_Rhode Island has weight of 0.7132069014875155\n",
      "state_South Carolina has weight of -0.31188418095108783\n",
      "state_South Dakota has weight of -0.639350984676371\n",
      "state_Tennessee has weight of 0.17929132280824997\n",
      "state_Texas has weight of -0.30319655149376246\n",
      "state_Utah has weight of -0.0007315298032498368\n",
      "state_Vermont has weight of -0.36124197378085343\n",
      "state_Virginia has weight of 0.3386983461883901\n",
      "state_Washington has weight of -0.8820214125618072\n",
      "state_West Virginia has weight of -0.9682271957192462\n",
      "state_Wisconsin has weight of -0.31726867385360963\n",
      "state_Wyoming has weight of -1.2004668013974016\n",
      "route_County Road has weight of -0.47920859471277216\n",
      "route_Interstate has weight of -0.508692194701474\n",
      "route_Local Street – Frontage Road (Since 1994) has weight of 0.4558428643347754\n",
      "route_Local Street – Municipality has weight of 0.7015047882176546\n",
      "route_Local Street – Township has weight of -0.23064678161570432\n",
      "route_State Highway has weight of -0.21885616928300536\n",
      "route_U.S. Highway has weight of -0.06490337840685326\n",
      "atmos_Blowing Sand, Soil, Dirt has weight of -0.19096353877392702\n",
      "atmos_Blowing Snow has weight of 0.3338076848960313\n",
      "atmos_Clear has weight of 0.27747564808222386\n",
      "atmos_Cloudy has weight of 0.1281273376089098\n",
      "atmos_Fog, Smog, Smoke has weight of -0.2827456272977245\n",
      "atmos_Freezing Rain or Drizzle has weight of -0.21819823803901833\n",
      "atmos_Rain has weight of 0.20416785899955994\n",
      "atmos_Severe Crosswinds has weight of -0.17358205112828967\n",
      "atmos_Sleet, Hail has weight of -0.15237128163631128\n",
      "atmos_Snow has weight of -0.2706772588690441\n",
      "light_Dark – Lighted has weight of 0.5935066610932643\n",
      "light_Dark – Not Lighted has weight of -0.3447834676930707\n",
      "light_Dark – Unknown Lighting has weight of -0.22175644138788034\n",
      "light_Dawn has weight of -0.18040749935913963\n",
      "light_Daylight has weight of -0.0480992226989399\n",
      "light_Dusk has weight of -0.14341949611370147\n"
     ]
    }
   ],
   "source": [
    "# Code addapted from the Dataming Notbooks. Logistic Regression Notbook 4.\n",
    "#Generates weights from the LR model.\n",
    "\n",
    "\n",
    "# iterate over the coefficients \n",
    "weights = OnTime_lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = Accident_forLr.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuses in meeting \n",
    "Weights changed to negative \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shayden\\AppData\\Local\\Continuum\\anaconda32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\shayden\\AppData\\Local\\Continuum\\anaconda32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\shayden\\AppData\\Local\\Continuum\\anaconda32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6420802175390891\n",
      "[[1269  433]\n",
      " [ 620  620]]\n",
      "light_Dark – Not Lighted has weight of -0.14756896158271898\n",
      "route_Interstate has weight of -0.13277597874554603\n",
      "route_County Road has weight of -0.13037491080482846\n",
      "state_Alabama has weight of -0.1271116573306914\n",
      "state_Montana has weight of -0.12299675878914317\n",
      "state_Wyoming has weight of -0.11894205864156975\n",
      "state_Missouri has weight of -0.11187085229816818\n",
      "state_West Virginia has weight of -0.09187993904194999\n",
      "state_Texas has weight of -0.09155488357242443\n",
      "state_Arkansas has weight of -0.08371292448614015\n",
      "state_Washington has weight of -0.08335749334123224\n",
      "state_Louisiana has weight of -0.07674975516918112\n",
      "state_New Hampshire has weight of -0.07367692503372274\n",
      "state_North Dakota has weight of -0.07107941487108205\n",
      "state_Kansas has weight of -0.06595680702162457\n",
      "atmos_Snow has weight of -0.055533751307331965\n",
      "state_South Dakota has weight of -0.054790895388901806\n",
      "state_Wisconsin has weight of -0.05286245319189313\n",
      "route_State Highway has weight of -0.05193887723346503\n",
      "atmos_Fog, Smog, Smoke has weight of -0.04535910009132023\n",
      "state_New Mexico has weight of -0.04376926374200487\n",
      "state_South Carolina has weight of -0.040706769336089806\n",
      "state_Oklahoma has weight of -0.04069189402590348\n",
      "state_Vermont has weight of -0.026319980877117823\n",
      "state_New Jersey has weight of -0.022854611903675805\n",
      "route_Local Street – Township has weight of -0.02277829076281523\n",
      "light_Dawn has weight of -0.02227058737422097\n",
      "state_Idaho has weight of -0.01960169623366454\n",
      "light_Dusk has weight of -0.01944293503553495\n",
      "atmos_Sleet, Hail has weight of -0.01939076122054396\n",
      "atmos_Cloudy has weight of -0.01912223493791331\n",
      "state_Iowa has weight of -0.018569933740704155\n",
      "light_Daylight has weight of -0.017369441401569985\n",
      "atmos_Severe Crosswinds has weight of -0.016713206750029762\n",
      "atmos_Freezing Rain or Drizzle has weight of -0.01436997154245408\n",
      "light_Dark – Unknown Lighting has weight of -0.013346709809125498\n",
      "state_Alaska has weight of -0.013320106632940526\n",
      "atmos_Blowing Sand, Soil, Dirt has weight of -0.012296597630097456\n",
      "state_Nebraska has weight of -0.011389744252258863\n",
      "state_California has weight of -0.011087732715202109\n",
      "state_Utah has weight of -0.006323096035118585\n",
      "state_Maine has weight of -0.0061912231487682885\n",
      "state_Maryland has weight of -0.001033637864520396\n",
      "atmos_Rain has weight of 0.005652527053215561\n",
      "state_Kentucky has weight of 0.0061028680917775404\n",
      "state_Mississippi has weight of 0.007958950066290495\n",
      "atmos_Blowing Snow has weight of 0.00933388209798726\n",
      "state_Illinois has weight of 0.015088626539451587\n",
      "state_Ohio has weight of 0.015475676639311438\n",
      "route_U.S. Highway has weight of 0.017855134838829785\n",
      "state_Virginia has weight of 0.0189525565440323\n",
      "state_Tennessee has weight of 0.020828401543927396\n",
      "state_District of Columbia has weight of 0.024453222417461326\n",
      "state_Colorado has weight of 0.02951439111452164\n",
      "state_Florida has weight of 0.030608953996653566\n",
      "state_Hawaii has weight of 0.033412824297488763\n",
      "state_Georgia has weight of 0.04134330450528672\n",
      "state_Rhode Island has weight of 0.041410446393297114\n",
      "atmos_Clear has weight of 0.042994481026555544\n",
      "route_Local Street – Frontage Road (Since 1994) has weight of 0.04888299806258596\n",
      "state_Massachusetts has weight of 0.05084213016337823\n",
      "state_Minnesota has weight of 0.05116018794421588\n",
      "state_Arizona has weight of 0.05405990488198127\n",
      "state_Nevada has weight of 0.05456471729373897\n",
      "state_Oregon has weight of 0.05651302493630616\n",
      "state_Delaware has weight of 0.0805734487026094\n",
      "state_Michigan has weight of 0.0890486038294323\n",
      "state_Connecticut has weight of 0.0962310967875985\n",
      "state_Pennsylvania has weight of 0.12583970106149042\n",
      "state_North Carolina has weight of 0.12764868125847859\n",
      "state_New York has weight of 0.15614082462059348\n",
      "light_Dark – Lighted has weight of 0.22101735418112137\n",
      "route_Local Street – Municipality has weight of 0.29613538722435107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "OnTime_lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "OnTime_lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = OnTime_lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(OnTime_lr_clf.coef_.T,Accident_forLr.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights based on the Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#traffic might suck more than living in the country??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
